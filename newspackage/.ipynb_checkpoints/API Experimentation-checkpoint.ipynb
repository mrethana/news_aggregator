{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json \n",
    "import requests\n",
    "import datetime\n",
    "from datetime import date\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('max_colwidth', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "newsapi = NewsApiClient(api_key='a539d7df2c7b43e1ac4d12f386d901e8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = newsapi.get_top_headlines(country='us', page_size=50, category = 'health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_api_key = '13bd501bc77542a58e2e6678619b0d60'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = int(str(date.today()).replace('-',''))\n",
    "last_week = int(str(date.today() - datetime.timedelta(days = 14)).replace('-',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYT & NewsAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import date\n",
    "from apikeys import *\n",
    "from info import *\n",
    "\n",
    "#dates to use for API call\n",
    "today = int(str(date.today()).replace('-',''))\n",
    "last_week = int(str(date.today() - datetime.timedelta(days = 14)).replace('-',''))\n",
    "\n",
    "#NEW YORK TIMES\n",
    "#clean the response from NYT API\n",
    "def NYT_title_clean(df):\n",
    "    titles = []\n",
    "    for index, row in df.iterrows():\n",
    "        title = row.headline['main']\n",
    "        titles.append(title)\n",
    "    df['title'] = titles\n",
    "    return df\n",
    "\n",
    "def NYT_dropped_rows(df):\n",
    "    df.pub_date = pd.to_datetime(df.pub_date).dt.date\n",
    "    df.word_count = round(df.word_count / 150)\n",
    "    df.document_type = 'text'\n",
    "    df['formality'] = 'Intermediate'\n",
    "    return df\n",
    "\n",
    "def NYT_dataframe_clean(df):\n",
    "    dataframe = NYT_title_clean(df)\n",
    "    dataframe = NYT_dropped_rows(dataframe)\n",
    "    return dataframe\n",
    "\n",
    "def NYT_api_call_section_based(section, source, page, start, end, key):\n",
    "    url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?fq=section_name:({section_name})&page={page}&source:({source})&begin_date={start}&end_date={end}&api-key={api}'.format(section_name = section, page = page, source = source, start = start, end = end, api = key)\n",
    "    resp = requests.get(url=url)\n",
    "    data = json.loads(resp.text)\n",
    "    df = pd.DataFrame(data['response']['docs'])\n",
    "    df = NYT_dataframe_clean(df)\n",
    "    df['param'] = section\n",
    "    df['image_url'] = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSHEtiVXw8Wi1tp56Nzd5rH_EoOAJA2RInEWvf5h5CQ-6O_YZp7dw'\n",
    "    return df\n",
    "\n",
    "def NYT_api_call_parameter_ALLTIME(param, page, key):\n",
    "    url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q={param}&page={page}&sort=newest&&api-key={api}'.format(param = param, page = page, api = key)\n",
    "    resp = requests.get(url=url)\n",
    "    data = json.loads(resp.text)\n",
    "    df = pd.DataFrame(data['response']['docs'])\n",
    "    df = NYT_dataframe_clean(df)\n",
    "    df['param'] = param\n",
    "    df['image_url'] = 'https://greaterbostonhcs.com/wp-content/uploads/2016/05/Nutrition.jpg'\n",
    "    return df\n",
    "\n",
    "def NYT_pull(categories):\n",
    "    empty = pd.DataFrame()\n",
    "    for word in categories:\n",
    "        try:\n",
    "            df = NYT_api_call_parameter_ALLTIME(word,0,nyt_api_key)\n",
    "            empty = empty.append(df, sort=True)\n",
    "            print('Pulled '+word)\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(word + \" EXCEPTION!!!!\")\n",
    "    empty = empty.drop(['abstract','section_name'],axis = 1)\n",
    "    empty = empty.rename(index=str, columns={\"_id\": \"source_id\", \"document_type\": \"medium\",'pub_date':'date','snippet':'description','word_count':'length'})\n",
    "    return empty\n",
    "\n",
    "#NEWSAPI\n",
    "\n",
    "def rename_columns(df):\n",
    "    df = df.rename(index=str, columns={'publishedAt':'date','url':'web_url','urlToImage':'image_url'})\n",
    "    return df\n",
    "\n",
    "def add_words(df):\n",
    "    lengths = []\n",
    "    for string in df.content:\n",
    "        try:\n",
    "            lengths.append(round(int(string[string.find('+')+1:string.find(' chars')]) / 4 / 250))\n",
    "        except:\n",
    "            lengths.append(4)\n",
    "    return lengths\n",
    "\n",
    "def split_source_info(list_of_dicts):\n",
    "    for item in list_of_dicts:\n",
    "        item['source_id'] = item['source']['id']\n",
    "        item['source'] = item['source']['name']\n",
    "\n",
    "def pull_articles(parameter):\n",
    "    article_results_rel = newsapi.get_everything(q=parameter,sort_by = 'relevancy',language='en', page_size=10, sources=sources_joined)\n",
    "    article_results_rel = article_results_rel['articles']\n",
    "    split_source_info(article_results_rel)\n",
    "    return article_results_rel\n",
    "\n",
    "def clean_articles(list_of_dicts, search_param):\n",
    "    df = pd.DataFrame(list_of_dicts)\n",
    "    try:\n",
    "        df['medium'] = 'text'\n",
    "        df['param'] = search_param\n",
    "        df['publishedAt'] = df['publishedAt'].apply(lambda x: pd.to_datetime(x).date().strftime('%Y-%m-%d'))\n",
    "        df['formality'] = 'Intermediate'\n",
    "        df['length'] = add_words(df)\n",
    "        df = rename_columns(df)\n",
    "        print(search_param)\n",
    "    except:\n",
    "        pass\n",
    "    return df\n",
    "\n",
    "\n",
    "def call_news_api(categories):\n",
    "    empty_df = pd.DataFrame()\n",
    "    for category in categories:\n",
    "        dicts = pull_articles(category)\n",
    "        df = clean_articles(dicts, category)\n",
    "        try:\n",
    "            empty_df = empty_df.append(df, sort=True)\n",
    "        except:\n",
    "            pass\n",
    "        print(len(empty_df.index))\n",
    "    return empty_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from apikeys import *\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import date\n",
    "from info import *\n",
    "\n",
    "auth = tweepy.OAuthHandler(twitter_1, twitter_2)\n",
    "auth.set_access_token(twitter_3, twitter_4)\n",
    "api = tweepy.API(auth)\n",
    "#all twitter handles to scrape\n",
    "# twitter_handles = ['@ATPScience1', '@waitrose', '@MicrobiomeInst', '@veganrecipescom', '@cldiet', '@Onnit', '@vegsoc', '@VeganKosher', '@TheVeganSociety', '@vegan', '@Keto_Recipes_', '@the52diet', '@IFdiet', '@microbiome', '@metagenomics', '@microbiome_news', '@TheGutStuff', '@MyGutHealth', '@PaleoFX',\n",
    "# '@PaleoFoundation', '@ThePaleoDiet', '@PaleoComfort', '@cavemanketo', '@KetoFlu', '@TheKetoKitchen_', '@EatKetoWithMe', '@KetoConnect', '@KetoDietZone', '@Ketogenic', '@USDANutrition', '@FoodRev', '@CSPI', '@simplyrecipes', '@FoodNetwork', '@CookingChannel', '@tasty', '@nytfood', '@finecooking', '@mrcookingpanda'\n",
    "# , '@FODMAPeveryday', '@FODMAPLife', '@FodmappedInfo', '@thefodmapdoctor', '@SimplyGlutenFre', '@gfliving', '@sibotest', '@manjulaskitchen', '@VegTimes', '@CookingLight', '@mealprepwl', '@thehealthygut', '@VitalGutHealth', '@pureguthealth', '@PaleoForBegin', '@PaleoLeap', '@ThePaleoMom', '@paleomagazine', '@PaleoHacks', '@paleogrubs',\n",
    "# '@naturalgourmet', '@Low_Carb_Keto', '@NutritionTwins', '@mckelhill', '@WomensFitnessAu', '@WomensHealthMag', '@MensHealthMag', '@mjfit', '@thugkitchen', '@Leslie_Klenke', '@insidePN', '@ThisMamaCooks', '@EdibleWildFood', '@TheEarthDieter', '@HarvardHealth', '@EverydayHealth', '@DailyHealthTips']\n",
    "\n",
    "#clean response from twitter\n",
    "\n",
    "def clean_tweets(data, categories):\n",
    "    tweets = []\n",
    "    for tweet in data:\n",
    "        try:\n",
    "            hashtag = tweet.entities['hashtags'][0]['text']\n",
    "            tags = list(pd.DataFrame(tweet.entities['hashtags']).text)\n",
    "            intersect = list(set(tags).intersection(categories))\n",
    "            if len(intersect) > 0:\n",
    "                hashtag = intersect[0]\n",
    "            else:\n",
    "                hashtag = hashtag\n",
    "        except IndexError:\n",
    "            hashtag = 'general'\n",
    "        tweets = [{'title':tweet.id, 'date':tweet.created_at.date().strftime('%Y-%m-%d'),\n",
    "           'description': tweet.text, 'source':tweet.user.screen_name,'source_id':tweet.user.id_str,\n",
    "           'formality': 'Informal'\n",
    "           ,'length': 1,'medium':'text', 'param':hashtag} for tweet in data]\n",
    "    tweets = pd.DataFrame(tweets)\n",
    "    return tweets\n",
    "\n",
    "#CALL API\n",
    "def twitter_api_call(list_handles, categories):\n",
    "    empty = pd.DataFrame()\n",
    "    for handle in list_handles:\n",
    "        user_tweets = pd.DataFrame(clean_tweets(api.user_timeline(handle), categories))\n",
    "        empty = empty.append(user_tweets, sort=True)\n",
    "        print(handle)\n",
    "    empty.title = empty.title.astype('str')\n",
    "    empty['web_url'] = 'https://twitter.com/'+empty.source+'/status/'+empty.title\n",
    "    empty['image_url'] = empty['web_url']\n",
    "    return empty\n",
    "import requests\n",
    "\n",
    "class Tweet(object):\n",
    "    def __init__(self, s, embed_str=False):\n",
    "        if not embed_str:\n",
    "            # Use Twitter's oEmbed API\n",
    "            # https://dev.twitter.com/web/embedded-tweets\n",
    "            api = 'https://publish.twitter.com/oembed?url={}'.format(s)\n",
    "            response = requests.get(api)\n",
    "            self.text = response.json()[\"html\"]\n",
    "        else:\n",
    "            self.text = s\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        return self.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_data import youtube_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apiclient.discovery import build\n",
    "from apiclient.errors import HttpError\n",
    "from oauth2client.tools import argparser\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import pafy\n",
    "\n",
    "\n",
    "DEVELOPER_KEY = \"AIzaSyAaNgP2we496MH8caRHTmGy4i02DjQkfMI\"\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "\n",
    "def clean_youtube_time(string):\n",
    "    if 'H' in string:\n",
    "        minutes = int(string[string.find('H')+1:string.find('M')])\n",
    "        hours = int(string[string.find('T')+1:string.find('H')]) * 60\n",
    "        time = minutes + hours\n",
    "    else:\n",
    "        if 'M' in string:\n",
    "            time = int(string[string.find('T')+1:string.find('M')])\n",
    "        else:\n",
    "            time = 1\n",
    "    return time\n",
    "\n",
    "def youtube_search(q, max_results=10,order=\"date\", token=None, location=None, location_radius=None):\n",
    "\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,developerKey=DEVELOPER_KEY)\n",
    "\n",
    "    search_response = youtube.search().list(\n",
    "    q=q,\n",
    "    type=\"video\",\n",
    "    pageToken=token,\n",
    "    order = order,\n",
    "    part=\"id,snippet\", # Part signifies the different types of data you want\n",
    "    maxResults=max_results,\n",
    "    location=location,\n",
    "    locationRadius=location_radius).execute()\n",
    "\n",
    "    all_dicts = []\n",
    "\n",
    "    for search_result in search_response.get(\"items\", []):\n",
    "        if search_result[\"id\"][\"kind\"] == \"youtube#video\":\n",
    "\n",
    "            title = (search_result['snippet']['title'])\n",
    "\n",
    "            videoId = (search_result['id']['videoId'])\n",
    "\n",
    "            response = youtube.videos().list(\n",
    "            part='statistics, snippet, contentDetails',\n",
    "            id=search_result['id']['videoId']).execute()\n",
    "\n",
    "            channelId = (response['items'][0]['snippet']['channelId'])\n",
    "            channelTitle = (response['items'][0]['snippet']['channelTitle'])\n",
    "            categoryId = (response['items'][0]['snippet']['categoryId'])\n",
    "            favoriteCount = (response['items'][0]['statistics']['favoriteCount'])\n",
    "            viewCount = (response['items'][0]['statistics']['viewCount'])\n",
    "            date = pd.to_datetime((response['items'][0]['snippet']['publishedAt'])).date().strftime('%Y-%m-%d')\n",
    "            description = response['items'][0]['snippet']['localized']['description']\n",
    "            url = 'https://www.youtube.com/watch?v='+videoId\n",
    "            image_url = response['items'][0]['snippet']['thumbnails']['default']['url']\n",
    "            length = clean_youtube_time(response['items'][0]['contentDetails']['duration'])\n",
    "            \n",
    "        if 'commentCount' in response['items'][0]['statistics'].keys():\n",
    "            commentCount = (response['items'][0]['statistics']['commentCount'])\n",
    "        else:\n",
    "            commentCount = []\n",
    "\n",
    "        if 'tags' in response['items'][0]['snippet'].keys():\n",
    "            tags = (response['items'][0]['snippet']['tags'])\n",
    "        else:\n",
    "            tags = []\n",
    "\n",
    "        youtube_dict = {'tags':tags,'source_id': channelId,'source': channelTitle,'categoryId':categoryId,'title':title,'videoId':videoId,'viewCount':viewCount,'commentCount':commentCount,'favoriteCount':favoriteCount, \n",
    "                        'formality':'Intermediate', 'medium':'video','date':date, 'description': description, 'web_url':url, 'image_url':image_url, 'length':length}\n",
    "        all_dicts.append(youtube_dict)\n",
    "    return pd.DataFrame(all_dicts)\n",
    "\n",
    "def add_category(df, categories):\n",
    "    # cats = ['keto','ketogenic','paleo','paleolithic','vegan','vegetarian']\n",
    "    all_params = []\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            intersect = list(set(row.tags).intersection(categories))\n",
    "            if len(intersect) > 0:\n",
    "                category = intersect[0]\n",
    "            else:\n",
    "                category = row.tags[0]\n",
    "        except:\n",
    "            category = 'none'\n",
    "        all_params.append(category)\n",
    "    df['param'] = all_params\n",
    "    return df\n",
    "\n",
    "def youtube_api_call(list_accounts, categories):\n",
    "    empty_df = pd.DataFrame()\n",
    "    errors = []\n",
    "    for account in list_accounts:\n",
    "        try:\n",
    "            df = youtube_search(account)\n",
    "            df = add_length(df)\n",
    "            df = add_category(df, categories)\n",
    "            empty_df = empty_df.append(df, sort=True)\n",
    "            print(account)\n",
    "        except:\n",
    "            print(account + \" EXCEPTION!!!!\")\n",
    "    return empty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categoryId</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>formality</th>\n",
       "      <th>image_url</th>\n",
       "      <th>length</th>\n",
       "      <th>medium</th>\n",
       "      <th>source</th>\n",
       "      <th>source_id</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>videoId</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>web_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>2018-11-25</td>\n",
       "      <td>El venenoso art√≠culo en 'The New York Times' que no har√° ni pizca de gracia a Felipe VI y Letizia.\\n#Letizia #Felipe #TheNewYork\\nNota: Todas las im√°genes que se muestran en el video se agregan y agregan de fuentes respetables, que no son de nuestra propiedad.\\n\\n***Problemas de derechos de autor por favor cont√°ctenos por correo electr√≥nico. Eliminaremos el clip, el video o llegaremos a un acuerdo!***\\nCorreo electr√≥nico: anmang9826@gmail.com\\n\\nViendo m√°s en: https://goo.gl/dCC34g\\nViendo m√°s en G+: https://goo.gl/hb6QUV\\nSuscr√≠bete en: https://goo.gl/g1rS4N\\n\\n¬°Gracias por ver!</td>\n",
       "      <td>0</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>https://i.ytimg.com/vi/6jKWQ-EgnyE/default.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>video</td>\n",
       "      <td>Vidas Casas Reales</td>\n",
       "      <td>UC46HumkyJfSBRxXC-1DujLQ</td>\n",
       "      <td>[Casa Real, Realeza, Familia Real, Letizia, Felipe, Felipe VI y Letizia, Felipe y Letizia, The New York, Reina Letizia, Rey Felipe, Felipe VI]</td>\n",
       "      <td>El venenoso art√≠culo en 'The New York Times' que no har√° ni pizca de gracia a Felipe VI y Letizia</td>\n",
       "      <td>6jKWQ-EgnyE</td>\n",
       "      <td>7025</td>\n",
       "      <td>https://www.youtube.com/watch?v=6jKWQ-EgnyE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-11-24</td>\n",
       "      <td>Is Pedro Arreguin #EllenBarry ! #NYT #Illinois üá∫üá∏ üá≤üáΩ #Talent #Tv #üìª #Youtube !! üì∞ üìì üñä üè´ ‚òï !!</td>\n",
       "      <td>0</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>https://i.ytimg.com/vi/nCTcaOzFPqE/default.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>video</td>\n",
       "      <td>Pedro Rojas</td>\n",
       "      <td>UCVYBulGSXrlmkJbDTJpk3mw</td>\n",
       "      <td>[]</td>\n",
       "      <td>Hey, Can You Write This Way? The Coolest Talent Ever! #TheNewYorkTimes #Starbucks #EllenDegeneres üì∫</td>\n",
       "      <td>nCTcaOzFPqE</td>\n",
       "      <td>59</td>\n",
       "      <td>https://www.youtube.com/watch?v=nCTcaOzFPqE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>2018-11-20</td>\n",
       "      <td>–°—É-57 –Ω–µ –∏—Å—Ç—Ä–µ–±–∏—Ç–µ–ª—å –ø—è—Ç–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è | The New York Times –∏ —Ä–æ—Å—Å–∏–π—Å–∫–∞—è –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è | –ü—É—Ç–∏–Ω –∏ –ö—É—Ä–∏–ª—ã (–ö—Ä—ã–º).\\n\\n\\n\\n\\n#–°—É57 #TheNewYorkTimes #–ö—É—Ä–∏–ª—ã</td>\n",
       "      <td>0</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>https://i.ytimg.com/vi/GwBIE6ZhAZM/default.jpg</td>\n",
       "      <td>11</td>\n",
       "      <td>video</td>\n",
       "      <td>Golden Tengu TV</td>\n",
       "      <td>UCYaE_hhSNZgvxEApYVZ41DQ</td>\n",
       "      <td>[–ù–æ–≤–æ—Å—Ç–∏, –°—É-57, The New York Times, –ö—É—Ä–∏–ª—ã]</td>\n",
       "      <td>–°—É-57 –Ω–µ –∏—Å—Ç—Ä–µ–±–∏—Ç–µ–ª—å –ø—è—Ç–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è | The New York Times –∏ –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è | –ü—É—Ç–∏–Ω –∏ –ö—É—Ä–∏–ª—ã (–ö—Ä—ã–º).</td>\n",
       "      <td>GwBIE6ZhAZM</td>\n",
       "      <td>2268</td>\n",
       "      <td>https://www.youtube.com/watch?v=GwBIE6ZhAZM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>305</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>The New York Times: –û–ø—É–±–ª–∏–∫–æ–≤–∞–ª–æ –≥–ª—É–±–æ–∫–æ–µ —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ –†–û–°–°–ò–ô–°–ö–û–ô –î–ï–ó–ò–ù–§–û–†–ú–ê–¶–ò–ò. –û –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –°–ú–ò.\\nThe New York Times: https://www.nytimes.com/2018/11/12/opinion/russia-meddling-disinformation-fake-news-elections.html?fbclid=IwAR2YPkmaLvGVA-8tB_TRlFOSiD3U0NdxuRcP3HS4_MyMSJX6xCwtpZUH1bg\\n\\n\\n#The New York Times #–î–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è #–†–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ</td>\n",
       "      <td>0</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>https://i.ytimg.com/vi/wg4M3ood9jo/default.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>video</td>\n",
       "      <td>–¢—ç–Ω–≥—É</td>\n",
       "      <td>UCK7RnQ7iOtzfkBgj-A1XKeg</td>\n",
       "      <td>[–î–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, The New York Times, —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, –†–æ—Å—Å–∏—è]</td>\n",
       "      <td>The New York Times: –û–ø—É–±–ª–∏–∫–æ–≤–∞–ª–æ –≥–ª—É–±–æ–∫–æ–µ —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ –†–û–°–°–ò–ô–°–ö–û–ô –î–ï–ó–ò–ù–§–û–†–ú–ê–¶–ò–ò.</td>\n",
       "      <td>wg4M3ood9jo</td>\n",
       "      <td>23980</td>\n",
       "      <td>https://www.youtube.com/watch?v=wg4M3ood9jo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>Susanne Fowler, o scriitoare americanƒÉ care a vizitat Bucure»ôtiul, a scris √Æn The New York Times despre micii din Pia»õa Obor!\\n\\n‚û† AboneazƒÉ-te la canalul nostru: https://goo.gl/GwS8uL\\nUrmƒÉre≈üte cel mai iubit matinal ≈üi pe:\\n‚û† TV Antena 1, √Æn fiecare diminea»õa, de luni p√¢nƒÉ vineri, de la ora 8:00.\\n‚û† Site-ul ‚ÄûNeatza cu RƒÉzvan »ôi Dani‚Äù: https://a1.ro/neatza-cu-razvan-si-dani\\n‚û† liveVIDEO ≈üi emisiunea integralƒÉ, doar pe https://goo.gl/sqijpP\\n‚û† Facebook: https://www.facebook.com/neatzacurazvansidani/\\n\\nYouTube network: Antena TV Group</td>\n",
       "      <td>0</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>https://i.ytimg.com/vi/-JsZZX574W4/default.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>video</td>\n",
       "      <td>Neatza cu Razvan si Dani</td>\n",
       "      <td>UC5RFt2bGPzbBtJ5UZKUVs9Q</td>\n",
       "      <td>[invitati, mititei, obor, the new york times, Neatza, Neatza cu Razvan si Dani, Antena 1, Razvan Simion, Dani Otil, Flavia Mihasan, neata, scriitoare americana, straini, mancare]</td>\n",
       "      <td>Mititeii de Obor au ajuns √Æn The New York Times!</td>\n",
       "      <td>-JsZZX574W4</td>\n",
       "      <td>4933</td>\n",
       "      <td>https://www.youtube.com/watch?v=-JsZZX574W4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>374</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>Know More News with Adam Green\\n\\n‚ö†Ô∏èüõë‚úã STOP!!! Subscribe to my BACK-UP CHANNEL!!! ‚úãüõë‚ö†Ô∏è\\n                                üëá Know More News 2  üëá\\nhttps://www.youtube.com/channel/UCz3QRglH_VPC5fjd3vekSsg\\n\\nAlbert Bishai's YouTube:\\nhttps://www.youtube.com/channel/UCyBrb_jsxYTK9rEfqv4Z6Uw\\n\\nSupport Know More News\\nPatreon - https://www.patreon.com/AdamGreen\\nPaypal Donations/Tips - https://www.paypal.me/KnowMoreNews\\nVenmo - @Know-More-News\\nTwitter - https://twitter.com/Know_More_News\\nFacebook - https://www.facebook.com/KnowMoreNews\\nSUBSCRIBE ON BITCHUTE!!!\\nhttps://www.bitchute.com/channel/know-more-news/\\n\\nGDL Shirts!\\nhttps://teespring.com/gdlshirt\\n\\nOfficial GDL Twitter\\nhttps://twitter.com/GDL__National</td>\n",
       "      <td>0</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>https://i.ytimg.com/vi/w6KYUeqrGjE/default.jpg</td>\n",
       "      <td>113</td>\n",
       "      <td>video</td>\n",
       "      <td>Know More News</td>\n",
       "      <td>UCaRjFptz7970mppuiGuZtpQ</td>\n",
       "      <td>[Kushner, Chabad, Jack Posbiec, The New York Times]</td>\n",
       "      <td>Kushner, Chabad, &amp; ZOA in The New York Times w/ Jack Posobiec</td>\n",
       "      <td>w6KYUeqrGjE</td>\n",
       "      <td>11194</td>\n",
       "      <td>https://www.youtube.com/watch?v=w6KYUeqrGjE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>358</td>\n",
       "      <td>2018-11-16</td>\n",
       "      <td>Anya Parampil shares details from a recent New York Times article which claims 16 secret missile sites have been discovered in North Korea, according to commercial satellite images. In Question Producer Kei Pritsker explains why we should be suspicious of the report, highlighting its inconsistencies and noting it‚Äôs based on information from the Center for Strategic and International Studies, a think tank funded by banks and weapons contractors.\\n\\nFind RT America in your area: http://rt.com/where-to-watch/\\nOr watch us online: http://rt.com/on-air/rt-america-air/\\n\\nLike us on Facebook http://www.facebook.com/RTAmerica\\nFollow us on Twitter http://twitter.com/RT_America</td>\n",
       "      <td>0</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>https://i.ytimg.com/vi/-nMMvY9zNrk/default.jpg</td>\n",
       "      <td>8</td>\n",
       "      <td>video</td>\n",
       "      <td>RT America</td>\n",
       "      <td>UCczrL-2b-gYK3l4yDld4XlQ</td>\n",
       "      <td>[Korea, Missile sites, North Korea, New York Times, NYT, Center for Strategic and International Studies, Kei Pritsker, DPRK, Anya Parampil, RT, RT America, Question More]</td>\n",
       "      <td>Correcting the New York Times‚Äô Fake News on North Korea</td>\n",
       "      <td>-nMMvY9zNrk</td>\n",
       "      <td>16509</td>\n",
       "      <td>https://www.youtube.com/watch?v=-nMMvY9zNrk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28</td>\n",
       "      <td>43</td>\n",
       "      <td>2018-11-10</td>\n",
       "      <td>Google Cloud is helping the New York Times digitize millions of photos from its archive. Picture what the cloud can do.\\n\\nSubscribe to the Google Cloud channel! ‚Üí http://bit.ly/2L5Xt72</td>\n",
       "      <td>0</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>https://i.ytimg.com/vi/bPX-9bTzqZQ/default.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>video</td>\n",
       "      <td>Google Cloud</td>\n",
       "      <td>UCTMRxtyHoE3LPcrl-kT4AQQ</td>\n",
       "      <td>[google cloud, new york times, google cloud partnership, digitizing new york times, google ny times partnership, google cloud platform, text classification, image recoginition, ml, macine learning, cloud ml, ml for cloud, archiving, digital archiving, g suite, tensorflow, google photos, google photos api]</td>\n",
       "      <td>Digitizing The New York Times archive with Google Cloud</td>\n",
       "      <td>bPX-9bTzqZQ</td>\n",
       "      <td>26712</td>\n",
       "      <td>https://www.youtube.com/watch?v=bPX-9bTzqZQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>51</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>Follow a New York Times reporter‚Äôs journey as she uncovers the stories of separated migrant families.\\n\\nLearn more: https://nyti.ms/2D8SC1s\\n\\nRead the full investigation: https://nyti.ms/2qOS9sI\\n\\nSubscribe to The Times: https://nyti.ms/2z9IX6f</td>\n",
       "      <td>0</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>https://i.ytimg.com/vi/ZWWkJCY2jDg/default.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>video</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>UCqnbDFdCpuN8CMEg0VuEBqA</td>\n",
       "      <td>[immigration, child separation, donald trump, trump, dhs, trump rally, immigrant caravan, caravan, border patrol, border security, child separation border, immigration news, nyt, nytimes, new york times, the new york times]</td>\n",
       "      <td>The Truth Is Worth It: Perseverance | The New York Times</td>\n",
       "      <td>ZWWkJCY2jDg</td>\n",
       "      <td>2109603</td>\n",
       "      <td>https://www.youtube.com/watch?v=ZWWkJCY2jDg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>140</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>Inside the 18-month New York Times investigation that revealed how the president engaged in dubious tax schemes in the 1990s.\\n\\nLearn more: https://nyti.ms/2D8SC1s\\n\\nRead the full investigation: https://nyti.ms/2xSIq8K\\n\\nSubscribe to The Times: https://nyti.ms/2z9IX6f</td>\n",
       "      <td>0</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>https://i.ytimg.com/vi/XWb_SDtiT3E/default.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>video</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>UCqnbDFdCpuN8CMEg0VuEBqA</td>\n",
       "      <td>[the new york times, donald trump, trump, nyt, nytimes, new york times, trump tax, trump taxes, donald trump money, trump news, fred trump, the art of the deal, art of the deal, trump rally]</td>\n",
       "      <td>The Truth Is Worth It: Rigor | The New York Times</td>\n",
       "      <td>XWb_SDtiT3E</td>\n",
       "      <td>3549666</td>\n",
       "      <td>https://www.youtube.com/watch?v=XWb_SDtiT3E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  categoryId commentCount        date  \\\n",
       "0         24           20  2018-11-25   \n",
       "1         22            0  2018-11-24   \n",
       "2         22           39  2018-11-20   \n",
       "3         25          305  2018-11-19   \n",
       "4         24            1  2018-11-19   \n",
       "5         25          374  2018-11-19   \n",
       "6         25          358  2018-11-16   \n",
       "7         28           43  2018-11-10   \n",
       "8         25           51  2018-10-30   \n",
       "9         25          140  2018-10-30   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       description  \\\n",
       "0                                                                                                                                       El venenoso art√≠culo en 'The New York Times' que no har√° ni pizca de gracia a Felipe VI y Letizia.\\n#Letizia #Felipe #TheNewYork\\nNota: Todas las im√°genes que se muestran en el video se agregan y agregan de fuentes respetables, que no son de nuestra propiedad.\\n\\n***Problemas de derechos de autor por favor cont√°ctenos por correo electr√≥nico. Eliminaremos el clip, el video o llegaremos a un acuerdo!***\\nCorreo electr√≥nico: anmang9826@gmail.com\\n\\nViendo m√°s en: https://goo.gl/dCC34g\\nViendo m√°s en G+: https://goo.gl/hb6QUV\\nSuscr√≠bete en: https://goo.gl/g1rS4N\\n\\n¬°Gracias por ver!   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Is Pedro Arreguin #EllenBarry ! #NYT #Illinois üá∫üá∏ üá≤üáΩ #Talent #Tv #üìª #Youtube !! üì∞ üìì üñä üè´ ‚òï !!   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           –°—É-57 –Ω–µ –∏—Å—Ç—Ä–µ–±–∏—Ç–µ–ª—å –ø—è—Ç–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è | The New York Times –∏ —Ä–æ—Å—Å–∏–π—Å–∫–∞—è –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è | –ü—É—Ç–∏–Ω –∏ –ö—É—Ä–∏–ª—ã (–ö—Ä—ã–º).\\n\\n\\n\\n\\n#–°—É57 #TheNewYorkTimes #–ö—É—Ä–∏–ª—ã   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                        The New York Times: –û–ø—É–±–ª–∏–∫–æ–≤–∞–ª–æ –≥–ª—É–±–æ–∫–æ–µ —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ –†–û–°–°–ò–ô–°–ö–û–ô –î–ï–ó–ò–ù–§–û–†–ú–ê–¶–ò–ò. –û –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –°–ú–ò.\\nThe New York Times: https://www.nytimes.com/2018/11/12/opinion/russia-meddling-disinformation-fake-news-elections.html?fbclid=IwAR2YPkmaLvGVA-8tB_TRlFOSiD3U0NdxuRcP3HS4_MyMSJX6xCwtpZUH1bg\\n\\n\\n#The New York Times #–î–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è #–†–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ   \n",
       "4                                                                                                                                                                                      Susanne Fowler, o scriitoare americanƒÉ care a vizitat Bucure»ôtiul, a scris √Æn The New York Times despre micii din Pia»õa Obor!\\n\\n‚û† AboneazƒÉ-te la canalul nostru: https://goo.gl/GwS8uL\\nUrmƒÉre≈üte cel mai iubit matinal ≈üi pe:\\n‚û† TV Antena 1, √Æn fiecare diminea»õa, de luni p√¢nƒÉ vineri, de la ora 8:00.\\n‚û† Site-ul ‚ÄûNeatza cu RƒÉzvan »ôi Dani‚Äù: https://a1.ro/neatza-cu-razvan-si-dani\\n‚û† liveVIDEO ≈üi emisiunea integralƒÉ, doar pe https://goo.gl/sqijpP\\n‚û† Facebook: https://www.facebook.com/neatzacurazvansidani/\\n\\nYouTube network: Antena TV Group   \n",
       "5  Know More News with Adam Green\\n\\n‚ö†Ô∏èüõë‚úã STOP!!! Subscribe to my BACK-UP CHANNEL!!! ‚úãüõë‚ö†Ô∏è\\n                                üëá Know More News 2  üëá\\nhttps://www.youtube.com/channel/UCz3QRglH_VPC5fjd3vekSsg\\n\\nAlbert Bishai's YouTube:\\nhttps://www.youtube.com/channel/UCyBrb_jsxYTK9rEfqv4Z6Uw\\n\\nSupport Know More News\\nPatreon - https://www.patreon.com/AdamGreen\\nPaypal Donations/Tips - https://www.paypal.me/KnowMoreNews\\nVenmo - @Know-More-News\\nTwitter - https://twitter.com/Know_More_News\\nFacebook - https://www.facebook.com/KnowMoreNews\\nSUBSCRIBE ON BITCHUTE!!!\\nhttps://www.bitchute.com/channel/know-more-news/\\n\\nGDL Shirts!\\nhttps://teespring.com/gdlshirt\\n\\nOfficial GDL Twitter\\nhttps://twitter.com/GDL__National   \n",
       "6                                           Anya Parampil shares details from a recent New York Times article which claims 16 secret missile sites have been discovered in North Korea, according to commercial satellite images. In Question Producer Kei Pritsker explains why we should be suspicious of the report, highlighting its inconsistencies and noting it‚Äôs based on information from the Center for Strategic and International Studies, a think tank funded by banks and weapons contractors.\\n\\nFind RT America in your area: http://rt.com/where-to-watch/\\nOr watch us online: http://rt.com/on-air/rt-america-air/\\n\\nLike us on Facebook http://www.facebook.com/RTAmerica\\nFollow us on Twitter http://twitter.com/RT_America   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Google Cloud is helping the New York Times digitize millions of photos from its archive. Picture what the cloud can do.\\n\\nSubscribe to the Google Cloud channel! ‚Üí http://bit.ly/2L5Xt72   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Follow a New York Times reporter‚Äôs journey as she uncovers the stories of separated migrant families.\\n\\nLearn more: https://nyti.ms/2D8SC1s\\n\\nRead the full investigation: https://nyti.ms/2qOS9sI\\n\\nSubscribe to The Times: https://nyti.ms/2z9IX6f   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Inside the 18-month New York Times investigation that revealed how the president engaged in dubious tax schemes in the 1990s.\\n\\nLearn more: https://nyti.ms/2D8SC1s\\n\\nRead the full investigation: https://nyti.ms/2xSIq8K\\n\\nSubscribe to The Times: https://nyti.ms/2z9IX6f   \n",
       "\n",
       "  favoriteCount     formality                                       image_url  \\\n",
       "0             0  Intermediate  https://i.ytimg.com/vi/6jKWQ-EgnyE/default.jpg   \n",
       "1             0  Intermediate  https://i.ytimg.com/vi/nCTcaOzFPqE/default.jpg   \n",
       "2             0  Intermediate  https://i.ytimg.com/vi/GwBIE6ZhAZM/default.jpg   \n",
       "3             0  Intermediate  https://i.ytimg.com/vi/wg4M3ood9jo/default.jpg   \n",
       "4             0  Intermediate  https://i.ytimg.com/vi/-JsZZX574W4/default.jpg   \n",
       "5             0  Intermediate  https://i.ytimg.com/vi/w6KYUeqrGjE/default.jpg   \n",
       "6             0  Intermediate  https://i.ytimg.com/vi/-nMMvY9zNrk/default.jpg   \n",
       "7             0  Intermediate  https://i.ytimg.com/vi/bPX-9bTzqZQ/default.jpg   \n",
       "8             0  Intermediate  https://i.ytimg.com/vi/ZWWkJCY2jDg/default.jpg   \n",
       "9             0  Intermediate  https://i.ytimg.com/vi/XWb_SDtiT3E/default.jpg   \n",
       "\n",
       "   length medium                    source                 source_id  \\\n",
       "0       5  video        Vidas Casas Reales  UC46HumkyJfSBRxXC-1DujLQ   \n",
       "1       3  video               Pedro Rojas  UCVYBulGSXrlmkJbDTJpk3mw   \n",
       "2      11  video           Golden Tengu TV  UCYaE_hhSNZgvxEApYVZ41DQ   \n",
       "3       3  video                     –¢—ç–Ω–≥—É  UCK7RnQ7iOtzfkBgj-A1XKeg   \n",
       "4       5  video  Neatza cu Razvan si Dani  UC5RFt2bGPzbBtJ5UZKUVs9Q   \n",
       "5     113  video            Know More News  UCaRjFptz7970mppuiGuZtpQ   \n",
       "6       8  video                RT America  UCczrL-2b-gYK3l4yDld4XlQ   \n",
       "7       3  video              Google Cloud  UCTMRxtyHoE3LPcrl-kT4AQQ   \n",
       "8       1  video        The New York Times  UCqnbDFdCpuN8CMEg0VuEBqA   \n",
       "9       1  video        The New York Times  UCqnbDFdCpuN8CMEg0VuEBqA   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                 tags  \\\n",
       "0                                                                                                                                                                      [Casa Real, Realeza, Familia Real, Letizia, Felipe, Felipe VI y Letizia, Felipe y Letizia, The New York, Reina Letizia, Rey Felipe, Felipe VI]   \n",
       "1                                                                                                                                                                                                                                                                                                                  []   \n",
       "2                                                                                                                                                                                                                                                                        [–ù–æ–≤–æ—Å—Ç–∏, –°—É-57, The New York Times, –ö—É—Ä–∏–ª—ã]   \n",
       "3                                                                                                                                                                                                                                                          [–î–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, The New York Times, —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, –†–æ—Å—Å–∏—è]   \n",
       "4                                                                                                                                  [invitati, mititei, obor, the new york times, Neatza, Neatza cu Razvan si Dani, Antena 1, Razvan Simion, Dani Otil, Flavia Mihasan, neata, scriitoare americana, straini, mancare]   \n",
       "5                                                                                                                                                                                                                                                                 [Kushner, Chabad, Jack Posbiec, The New York Times]   \n",
       "6                                                                                                                                          [Korea, Missile sites, North Korea, New York Times, NYT, Center for Strategic and International Studies, Kei Pritsker, DPRK, Anya Parampil, RT, RT America, Question More]   \n",
       "7  [google cloud, new york times, google cloud partnership, digitizing new york times, google ny times partnership, google cloud platform, text classification, image recoginition, ml, macine learning, cloud ml, ml for cloud, archiving, digital archiving, g suite, tensorflow, google photos, google photos api]   \n",
       "8                                                                                     [immigration, child separation, donald trump, trump, dhs, trump rally, immigrant caravan, caravan, border patrol, border security, child separation border, immigration news, nyt, nytimes, new york times, the new york times]   \n",
       "9                                                                                                                      [the new york times, donald trump, trump, nyt, nytimes, new york times, trump tax, trump taxes, donald trump money, trump news, fred trump, the art of the deal, art of the deal, trump rally]   \n",
       "\n",
       "                                                                                                 title  \\\n",
       "0    El venenoso art√≠culo en 'The New York Times' que no har√° ni pizca de gracia a Felipe VI y Letizia   \n",
       "1  Hey, Can You Write This Way? The Coolest Talent Ever! #TheNewYorkTimes #Starbucks #EllenDegeneres üì∫   \n",
       "2  –°—É-57 –Ω–µ –∏—Å—Ç—Ä–µ–±–∏—Ç–µ–ª—å –ø—è—Ç–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è | The New York Times –∏ –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è | –ü—É—Ç–∏–Ω –∏ –ö—É—Ä–∏–ª—ã (–ö—Ä—ã–º).   \n",
       "3                  The New York Times: –û–ø—É–±–ª–∏–∫–æ–≤–∞–ª–æ –≥–ª—É–±–æ–∫–æ–µ —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ –†–û–°–°–ò–ô–°–ö–û–ô –î–ï–ó–ò–ù–§–û–†–ú–ê–¶–ò–ò.   \n",
       "4                                                     Mititeii de Obor au ajuns √Æn The New York Times!   \n",
       "5                                        Kushner, Chabad, & ZOA in The New York Times w/ Jack Posobiec   \n",
       "6                                              Correcting the New York Times‚Äô Fake News on North Korea   \n",
       "7                                              Digitizing The New York Times archive with Google Cloud   \n",
       "8                                             The Truth Is Worth It: Perseverance | The New York Times   \n",
       "9                                                    The Truth Is Worth It: Rigor | The New York Times   \n",
       "\n",
       "       videoId viewCount                                      web_url  \n",
       "0  6jKWQ-EgnyE      7025  https://www.youtube.com/watch?v=6jKWQ-EgnyE  \n",
       "1  nCTcaOzFPqE        59  https://www.youtube.com/watch?v=nCTcaOzFPqE  \n",
       "2  GwBIE6ZhAZM      2268  https://www.youtube.com/watch?v=GwBIE6ZhAZM  \n",
       "3  wg4M3ood9jo     23980  https://www.youtube.com/watch?v=wg4M3ood9jo  \n",
       "4  -JsZZX574W4      4933  https://www.youtube.com/watch?v=-JsZZX574W4  \n",
       "5  w6KYUeqrGjE     11194  https://www.youtube.com/watch?v=w6KYUeqrGjE  \n",
       "6  -nMMvY9zNrk     16509  https://www.youtube.com/watch?v=-nMMvY9zNrk  \n",
       "7  bPX-9bTzqZQ     26712  https://www.youtube.com/watch?v=bPX-9bTzqZQ  \n",
       "8  ZWWkJCY2jDg   2109603  https://www.youtube.com/watch?v=ZWWkJCY2jDg  \n",
       "9  XWb_SDtiT3E   3549666  https://www.youtube.com/watch?v=XWb_SDtiT3E  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_search('@TheNewYorkTimes', max_results=10,order=\"date\", token=None, location=None, location_radius=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_youtube_time(string):\n",
    "    if 'H' in string:\n",
    "        minutes = int(string[string.find('H')+1:string.find('M')])\n",
    "        hours = int(string[string.find('T')+1:string.find('H')]) * 60\n",
    "        time = minutes + hours\n",
    "    else:\n",
    "        if 'M' in string:\n",
    "            time = int(string[string.find('T')+1:string.find('M')])\n",
    "        else:\n",
    "            time = 1\n",
    "    return time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FB AND INSTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from instagram.client import InstagramAPI\n",
    "# api = InstagramAPI(client_id=\"EAACl5okwUhQBAD6vhJiELgsatruedytMV67mvDuN2wgXEyRAXG7umE3T0KEweMhlWQWPgku5pF8KBwwOy9JJuFxx2chbYxPTjMdYA2mTd1DL6Jd5t4XwcetZBwhZC1Pyrl5MW2X90w6J9ZCZCWYlODApHzosU7ReUPZBmFITJj0cpsMZCDZATmEKip23ZA1nyA40Dv8IsLccLAZDZD\")\n",
    "# popular_media = api.media_popular(count=20)\n",
    "# for media in popular_media:\n",
    "#     print(media.images['standard_resolution'].url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import facebook\n",
    "\n",
    "graph = facebook.GraphAPI(access_token=\"EAACl5okwUhQBANPTECp6sKqxm3vubZCFmBBEtwdUQgLPAvAFLhpmk6ZAgDTB6Q6g6y6lvz3yjf7iZBW2ELCSrAwDhp28A1xse1QkQrvXowYgivMP0Rz3NRO8cEEy514LC7x3pAmFr1RBZBe7RJVVlsY70OWLHOgceSZCZC0HqzFdw13oEkEZC0Bm4ehPBxTlJFOoyiFE5f8ZCAZDZD\", version=\"2.12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedUrlLinks(search_words, media_value='podcast', entity_value='podcast'):\n",
    "    \"\"\"\n",
    "    Fetches FeedUrl(s) of a search request made using the Itunes Api\n",
    "    \n",
    "    Args:\n",
    "        search_words: The URL-encoded text string to be searched for\n",
    "        media_value: {movie, podcast, music, musicVideo, audiobook,\n",
    "                    shortFilm, tvShow, software, ebook, all} optional\n",
    "                    An optional variable, which indicates the media type to be searched for.\n",
    "        entity_value: Optional\n",
    "        \n",
    "    Returns:\n",
    "        A list of feed urls\n",
    "        \n",
    "        example:\n",
    "        \n",
    "        ['https://www.npr.org/rss/podcast.php?id=381444908', \n",
    "        'http://www.spacemusic.nl/podcast/freshairlounge/freshairlounge.xml', \n",
    "        'http://feeds.soundcloud.com/users/soundcloud:users:192634952/sounds.rss', \n",
    "        'http://feeds.feedburner.com/FAIOR_vid', \n",
    "        'http://feeds.soundcloud.com/users/soundcloud:users:280924508/sounds.rss',\n",
    "        'http://feeds2.feedburner.com/faior', 'http://rss.acast.com/takingthepulse']\n",
    "\n",
    "    \"\"\"\n",
    "    payload = {'term': search_words, 'media': media_value, 'entity' : entity_value}\n",
    "    # make a http request with the payload as query parameters\n",
    "    itunes_request = requests.get('https://itunes.apple.com/search', params=payload)\n",
    "    # print Full request query\n",
    "    print(\"\\nYour Passed in query is ->\", itunes_request.url)\n",
    "    \n",
    "    # Store the json result of the Query\n",
    "    itunes_result_json = itunes_request.json()\n",
    "#     return itunes_result_json\n",
    "    #print(\"\\n\",itunes_result_json)\n",
    "    # Get the number of results count so we know what to loop through\n",
    "    result_count = itunes_result_json[\"resultCount\"]\n",
    "    feed_url_list = []\n",
    "    i = 0\n",
    "    while(i < result_count):\n",
    "        # get the Feed Url for each result\n",
    "        feed_url = itunes_result_json[\"results\"][i]['feedUrl']\n",
    "        feed_url_list.append(feed_url)\n",
    "        i = i + 1\n",
    "    return feed_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your Passed in query is -> https://itunes.apple.com/search?term=wellness+mama&media=podcast&entity=podcast\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://wellnessmama.com/feed/podcast',\n",
       " 'http://theenginemom.libsyn.com/rss',\n",
       " 'http://mamainthemaking.libsyn.com/rss',\n",
       " 'https://unconventionalwellnessradio.castos.com/feed']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedUrlLinks('wellness mama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your Passed in query is -> https://itunes.apple.com/search?term=wellness+mama&media=podcast&entity=podcast\n",
      "\n",
      " ['https://wellnessmama.com/feed/podcast', 'http://theenginemom.libsyn.com/rss', 'http://mamainthemaking.libsyn.com/rss', 'https://unconventionalwellnessradio.castos.com/feed']\n",
      "url name https://wellnessmama.com/feed/podcast\n",
      "url name http://theenginemom.libsyn.com/rss\n",
      "url name http://mamainthemaking.libsyn.com/rss\n",
      "url name https://unconventionalwellnessradio.castos.com/feed\n"
     ]
    }
   ],
   "source": [
    "# In the future ask the User for the name of the Podcast to be searched for\n",
    "# construct the Itunes search query to search for the Podcast\n",
    "\n",
    "\n",
    "feed_url_link_list = feedUrlLinks(\"wellness mama\")\n",
    "\n",
    "if(len(feed_url_link_list) > 0):\n",
    "    print(\"\\n\", feed_url_link_list)\n",
    "    i = 1\n",
    "    for url in feed_url_link_list:\n",
    "        feed = feedparser.parse(url)\n",
    "        # (feed.entries) is still the same as feed['entries']\n",
    "        print('url name', url)\n",
    "        sheet_list = []\n",
    "        for item in feed.entries:\n",
    "            info = dict()\n",
    "            info['title'] = item['title'] if 'title' in item else ''\n",
    "            info['Episode description']= item['summary'] if 'summary' in item else ''\n",
    "            info['Episode length']= item['itunes_duration'] if 'itunes_duration' in item else ''\n",
    "            info['Release date']= item['published'] if 'published' in item else ''\n",
    "            sheet_list.append(info)\n",
    "        df = pd.DataFrame(sheet_list)\n",
    "        i = i + 1\n",
    "else:\n",
    "    print(\"Search came back as empty, can try again with a different keyword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['itunes_title', 'title', 'title_detail', 'authors', 'author', 'author_detail', 'subtitle', 'subtitle_detail', 'summary', 'summary_detail', 'content', 'googleplay_description', 'links', 'id', 'guidislink', 'link', 'published', 'published_parsed'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed.entries[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time.struct_time(tm_year=2018, tm_mon=11, tm_mday=14, tm_hour=20, tm_min=53, tm_sec=0, tm_wday=2, tm_yday=318, tm_isdst=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed.entries[0]['published_parsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
