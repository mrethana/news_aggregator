{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json \n",
    "import requests\n",
    "import datetime\n",
    "from datetime import date\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('max_colwidth', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "newsapi = NewsApiClient(api_key='a539d7df2c7b43e1ac4d12f386d901e8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = newsapi.get_top_headlines(country='us', page_size=50, category = 'health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_api_key = '13bd501bc77542a58e2e6678619b0d60'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = int(str(date.today()).replace('-',''))\n",
    "last_week = int(str(date.today() - datetime.timedelta(days = 14)).replace('-',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYT & NewsAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import date\n",
    "from apikeys import *\n",
    "from info import *\n",
    "\n",
    "#dates to use for API call\n",
    "today = int(str(date.today()).replace('-',''))\n",
    "last_week = int(str(date.today() - datetime.timedelta(days = 14)).replace('-',''))\n",
    "\n",
    "#NEW YORK TIMES\n",
    "#clean the response from NYT API\n",
    "def NYT_title_clean(df):\n",
    "    titles = []\n",
    "    for index, row in df.iterrows():\n",
    "        title = row.headline['main']\n",
    "        titles.append(title)\n",
    "    df['title'] = titles\n",
    "    return df\n",
    "\n",
    "def NYT_dropped_rows(df):\n",
    "    df.pub_date = pd.to_datetime(df.pub_date).dt.date\n",
    "    df.word_count = round(df.word_count / 150)\n",
    "    df.document_type = 'text'\n",
    "    df['formality'] = 'Intermediate'\n",
    "    return df\n",
    "\n",
    "def NYT_dataframe_clean(df):\n",
    "    dataframe = NYT_title_clean(df)\n",
    "    dataframe = NYT_dropped_rows(dataframe)\n",
    "    return dataframe\n",
    "\n",
    "def NYT_api_call_section_based(section, source, page, start, end, key):\n",
    "    url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?fq=section_name:({section_name})&page={page}&source:({source})&begin_date={start}&end_date={end}&api-key={api}'.format(section_name = section, page = page, source = source, start = start, end = end, api = key)\n",
    "    resp = requests.get(url=url)\n",
    "    data = json.loads(resp.text)\n",
    "    df = pd.DataFrame(data['response']['docs'])\n",
    "    df = NYT_dataframe_clean(df)\n",
    "    df['param'] = section\n",
    "    df['image_url'] = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSHEtiVXw8Wi1tp56Nzd5rH_EoOAJA2RInEWvf5h5CQ-6O_YZp7dw'\n",
    "    return df\n",
    "\n",
    "def NYT_api_call_parameter_ALLTIME(param, page, key):\n",
    "    url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q={param}&page={page}&sort=newest&&api-key={api}'.format(param = param, page = page, api = key)\n",
    "    resp = requests.get(url=url)\n",
    "    data = json.loads(resp.text)\n",
    "    df = pd.DataFrame(data['response']['docs'])\n",
    "    df = NYT_dataframe_clean(df)\n",
    "    df['param'] = param\n",
    "    df['image_url'] = 'https://greaterbostonhcs.com/wp-content/uploads/2016/05/Nutrition.jpg'\n",
    "    return df\n",
    "\n",
    "def NYT_pull(categories):\n",
    "    empty = pd.DataFrame()\n",
    "    for word in categories:\n",
    "        try:\n",
    "            df = NYT_api_call_parameter_ALLTIME(word,0,nyt_api_key)\n",
    "            empty = empty.append(df, sort=True)\n",
    "            print('Pulled '+word)\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(word + \" EXCEPTION!!!!\")\n",
    "    empty = empty.drop(['abstract','section_name'],axis = 1)\n",
    "    empty = empty.rename(index=str, columns={\"_id\": \"source_id\", \"document_type\": \"medium\",'pub_date':'date','snippet':'description','word_count':'length'})\n",
    "    return empty\n",
    "\n",
    "#NEWSAPI\n",
    "\n",
    "def rename_columns(df):\n",
    "    df = df.rename(index=str, columns={'publishedAt':'date','url':'web_url','urlToImage':'image_url'})\n",
    "    return df\n",
    "\n",
    "def add_words(df):\n",
    "    lengths = []\n",
    "    for string in df.content:\n",
    "        try:\n",
    "            lengths.append(round(int(string[string.find('+')+1:string.find(' chars')]) / 4 / 250))\n",
    "        except:\n",
    "            lengths.append(4)\n",
    "    return lengths\n",
    "\n",
    "def split_source_info(list_of_dicts):\n",
    "    for item in list_of_dicts:\n",
    "        item['source_id'] = item['source']['id']\n",
    "        item['source'] = item['source']['name']\n",
    "\n",
    "def pull_articles(parameter):\n",
    "    article_results_rel = newsapi.get_everything(q=parameter,sort_by = 'relevancy',language='en', page_size=10, sources=sources_joined)\n",
    "    article_results_rel = article_results_rel['articles']\n",
    "    split_source_info(article_results_rel)\n",
    "    return article_results_rel\n",
    "\n",
    "def clean_articles(list_of_dicts, search_param):\n",
    "    df = pd.DataFrame(list_of_dicts)\n",
    "    try:\n",
    "        df['medium'] = 'text'\n",
    "        df['param'] = search_param\n",
    "        df['publishedAt'] = df['publishedAt'].apply(lambda x: pd.to_datetime(x).date().strftime('%Y-%m-%d'))\n",
    "        df['formality'] = 'Intermediate'\n",
    "        df['length'] = add_words(df)\n",
    "        df = rename_columns(df)\n",
    "        print(search_param)\n",
    "    except:\n",
    "        pass\n",
    "    return df\n",
    "\n",
    "\n",
    "def call_news_api(categories):\n",
    "    empty_df = pd.DataFrame()\n",
    "    for category in categories:\n",
    "        dicts = pull_articles(category)\n",
    "        df = clean_articles(dicts, category)\n",
    "        try:\n",
    "            empty_df = empty_df.append(df, sort=True)\n",
    "        except:\n",
    "            pass\n",
    "        print(len(empty_df.index))\n",
    "    return empty_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from apikeys import *\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import date\n",
    "from info import *\n",
    "\n",
    "auth = tweepy.OAuthHandler(twitter_1, twitter_2)\n",
    "auth.set_access_token(twitter_3, twitter_4)\n",
    "api = tweepy.API(auth)\n",
    "#all twitter handles to scrape\n",
    "# twitter_handles = ['@ATPScience1', '@waitrose', '@MicrobiomeInst', '@veganrecipescom', '@cldiet', '@Onnit', '@vegsoc', '@VeganKosher', '@TheVeganSociety', '@vegan', '@Keto_Recipes_', '@the52diet', '@IFdiet', '@microbiome', '@metagenomics', '@microbiome_news', '@TheGutStuff', '@MyGutHealth', '@PaleoFX',\n",
    "# '@PaleoFoundation', '@ThePaleoDiet', '@PaleoComfort', '@cavemanketo', '@KetoFlu', '@TheKetoKitchen_', '@EatKetoWithMe', '@KetoConnect', '@KetoDietZone', '@Ketogenic', '@USDANutrition', '@FoodRev', '@CSPI', '@simplyrecipes', '@FoodNetwork', '@CookingChannel', '@tasty', '@nytfood', '@finecooking', '@mrcookingpanda'\n",
    "# , '@FODMAPeveryday', '@FODMAPLife', '@FodmappedInfo', '@thefodmapdoctor', '@SimplyGlutenFre', '@gfliving', '@sibotest', '@manjulaskitchen', '@VegTimes', '@CookingLight', '@mealprepwl', '@thehealthygut', '@VitalGutHealth', '@pureguthealth', '@PaleoForBegin', '@PaleoLeap', '@ThePaleoMom', '@paleomagazine', '@PaleoHacks', '@paleogrubs',\n",
    "# '@naturalgourmet', '@Low_Carb_Keto', '@NutritionTwins', '@mckelhill', '@WomensFitnessAu', '@WomensHealthMag', '@MensHealthMag', '@mjfit', '@thugkitchen', '@Leslie_Klenke', '@insidePN', '@ThisMamaCooks', '@EdibleWildFood', '@TheEarthDieter', '@HarvardHealth', '@EverydayHealth', '@DailyHealthTips']\n",
    "\n",
    "#clean response from twitter\n",
    "\n",
    "def clean_tweets(data, categories):\n",
    "    topics = []\n",
    "    for tweet in data:\n",
    "        try:\n",
    "            hashtag = tweet.entities['hashtags'][0]['text']\n",
    "            tags = list(pd.DataFrame(tweet.entities['hashtags']).text)\n",
    "            intersect = list(set(tags).intersection(categories))\n",
    "            if len(intersect) > 0:\n",
    "                hashtag = intersect[0]\n",
    "            else:\n",
    "                hashtag = hashtag\n",
    "        except IndexError:\n",
    "            words = set(re.sub(\"[^\\w]\", \" \",  tweet.text).split())\n",
    "            int2 = list(words.intersection(categories))\n",
    "            if len(int2) > 0:\n",
    "                hashtag = int2[0]\n",
    "            else:\n",
    "                hashtag = 'general'\n",
    "        topics.append(hashtag)\n",
    "    tweets = [{'title':tweet.id, 'date':tweet.created_at.date().strftime('%Y-%m-%d'),\n",
    "       'description': tweet.text, 'source':tweet.user.screen_name,'source_id':tweet.user.id_str,\n",
    "       'formality': 'Informal'\n",
    "       ,'length': 1,'medium':'text'} for tweet in data]\n",
    "    tweets = pd.DataFrame(tweets)\n",
    "    tweets['param'] = topics\n",
    "    return tweets\n",
    "\n",
    "\n",
    "\n",
    "#CALL API\n",
    "def twitter_api_call(list_handles, categories):\n",
    "    empty = pd.DataFrame()\n",
    "    for handle in list_handles:\n",
    "        user_tweets = pd.DataFrame(clean_tweets(api.user_timeline(handle), categories))\n",
    "        empty = empty.append(user_tweets, sort=True)\n",
    "        print(handle)\n",
    "    empty.title = empty.title.astype('str')\n",
    "    empty['web_url'] = 'https://twitter.com/'+empty.source+'/status/'+empty.title\n",
    "    empty['image_url'] = empty['web_url']\n",
    "    return empty\n",
    "import requests\n",
    "\n",
    "class Tweet(object):\n",
    "    def __init__(self, s, embed_str=False):\n",
    "        if not embed_str:\n",
    "            # Use Twitter's oEmbed API\n",
    "            # https://dev.twitter.com/web/embedded-tweets\n",
    "            api = 'https://publish.twitter.com/oembed?url={}'.format(s)\n",
    "            response = requests.get(api)\n",
    "            self.text = response.json()[\"html\"]\n",
    "        else:\n",
    "            self.text = s\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        return self.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apiclient.discovery import build\n",
    "from apiclient.errors import HttpError\n",
    "from oauth2client.tools import argparser\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import pafy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from apikeys import *\n",
    "from info import *\n",
    "\n",
    "\n",
    "DEVELOPER_KEY = youtube_key\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "\n",
    "\n",
    "def clean_youtube_time(string):\n",
    "    if 'H' in string:\n",
    "        minutes = int(string[string.find('H')+1:string.find('M')])\n",
    "        hours = int(string[string.find('T')+1:string.find('H')]) * 60\n",
    "        time = minutes + hours\n",
    "    else:\n",
    "        if 'M' in string:\n",
    "            time = int(string[string.find('T')+1:string.find('M')])\n",
    "        else:\n",
    "            time = 1\n",
    "    return time\n",
    "\n",
    "def youtube_search(q, max_results=10,order=\"date\", token=None, location=None, location_radius=None):\n",
    "\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,developerKey=DEVELOPER_KEY)\n",
    "\n",
    "    search_response = youtube.search().list(\n",
    "    q=q,\n",
    "    type=\"video\",\n",
    "    pageToken=token,\n",
    "    order = order,\n",
    "    part=\"id,snippet\", # Part signifies the different types of data you want\n",
    "    maxResults=max_results,\n",
    "    location=location,\n",
    "    locationRadius=location_radius).execute()\n",
    "\n",
    "    all_dicts = []\n",
    "\n",
    "    for search_result in search_response.get(\"items\", []):\n",
    "        if search_result[\"id\"][\"kind\"] == \"youtube#video\":\n",
    "\n",
    "            title = (search_result['snippet']['title'])\n",
    "\n",
    "            videoId = (search_result['id']['videoId'])\n",
    "\n",
    "            response = youtube.videos().list(\n",
    "            part='statistics, snippet, contentDetails',\n",
    "            id=search_result['id']['videoId']).execute()\n",
    "\n",
    "            channelId = (response['items'][0]['snippet']['channelId'])\n",
    "            channelTitle = (response['items'][0]['snippet']['channelTitle'])\n",
    "            categoryId = (response['items'][0]['snippet']['categoryId'])\n",
    "            favoriteCount = (response['items'][0]['statistics']['favoriteCount'])\n",
    "            viewCount = (response['items'][0]['statistics']['viewCount'])\n",
    "            date = pd.to_datetime((response['items'][0]['snippet']['publishedAt'])).date().strftime('%Y-%m-%d')\n",
    "            description = response['items'][0]['snippet']['localized']['description']\n",
    "            url = 'https://www.youtube.com/watch?v='+videoId\n",
    "            image_url = response['items'][0]['snippet']['thumbnails']['default']['url']\n",
    "            length = clean_youtube_time(response['items'][0]['contentDetails']['duration'])\n",
    "\n",
    "        if 'commentCount' in response['items'][0]['statistics'].keys():\n",
    "            commentCount = (response['items'][0]['statistics']['commentCount'])\n",
    "        else:\n",
    "            commentCount = []\n",
    "\n",
    "        if 'tags' in response['items'][0]['snippet'].keys():\n",
    "            tags = (response['items'][0]['snippet']['tags'])\n",
    "        else:\n",
    "            tags = []\n",
    "\n",
    "        youtube_dict = {'tags':tags,'source_id': channelId,'source': channelTitle,'categoryId':categoryId,'title':title,'videoId':videoId,'viewCount':viewCount,'commentCount':commentCount,'favoriteCount':favoriteCount,\n",
    "                        'formality':'Intermediate', 'medium':'video','date':date, 'description': description, 'web_url':url, 'image_url':image_url, 'length':length}\n",
    "        all_dicts.append(youtube_dict)\n",
    "    return pd.DataFrame(all_dicts)\n",
    "\n",
    "def add_category(df, categories):\n",
    "    # cats = ['keto','ketogenic','paleo','paleolithic','vegan','vegetarian']\n",
    "    all_params = []\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            intersect = list(set(row.tags).intersection(categories))\n",
    "            if len(intersect) > 0:\n",
    "                category = intersect[0]\n",
    "            else:\n",
    "                category = row.tags[0]\n",
    "        except:\n",
    "            category = 'none'\n",
    "        all_params.append(category)\n",
    "    df['param'] = all_params\n",
    "    return df\n",
    "\n",
    "def youtube_api_call(list_accounts, categories):\n",
    "    empty_df = pd.DataFrame()\n",
    "    errors = []\n",
    "    for account in list_accounts:\n",
    "        try:\n",
    "            df = youtube_search(account)\n",
    "            df = add_category(df, categories)\n",
    "            empty_df = empty_df.append(df, sort=True)\n",
    "            print(account)\n",
    "        except:\n",
    "            print(account + \" EXCEPTION!!!!\")\n",
    "    return empty_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FB AND INSTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from instagram.client import InstagramAPI\n",
    "# api = InstagramAPI(client_id=\"EAACl5okwUhQBAD6vhJiELgsatruedytMV67mvDuN2wgXEyRAXG7umE3T0KEweMhlWQWPgku5pF8KBwwOy9JJuFxx2chbYxPTjMdYA2mTd1DL6Jd5t4XwcetZBwhZC1Pyrl5MW2X90w6J9ZCZCWYlODApHzosU7ReUPZBmFITJj0cpsMZCDZATmEKip23ZA1nyA40Dv8IsLccLAZDZD\")\n",
    "# popular_media = api.media_popular(count=20)\n",
    "# for media in popular_media:\n",
    "#     print(media.images['standard_resolution'].url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import facebook\n",
    "\n",
    "graph = facebook.GraphAPI(access_token=\"EAACl5okwUhQBANPTECp6sKqxm3vubZCFmBBEtwdUQgLPAvAFLhpmk6ZAgDTB6Q6g6y6lvz3yjf7iZBW2ELCSrAwDhp28A1xse1QkQrvXowYgivMP0Rz3NRO8cEEy514LC7x3pAmFr1RBZBe7RJVVlsY70OWLHOgceSZCZC0HqzFdw13oEkEZC0Bm4ehPBxTlJFOoyiFE5f8ZCAZDZD\", version=\"2.12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "def feed_urls(search_words, media_value='podcast', entity_value='podcast'):\n",
    "#     Args:\n",
    "#         search_words: The URL-encoded text string to be searched for\n",
    "#         media_value: {movie, podcast, music, musicVideo, audiobook,\n",
    "#                     shortFilm, tvShow, software, ebook, all} optional\n",
    "#                     An optional variable, which indicates the media type to be searched for.\n",
    "#         entity_value: Optional\n",
    "\n",
    "    payload = {'term': search_words, 'media': media_value, 'entity' : entity_value}\n",
    "    itunes_request = requests.get('https://itunes.apple.com/search', params=payload)\n",
    "    itunes_result_json = itunes_request.json()\n",
    "    result_count = itunes_result_json[\"resultCount\"]\n",
    "    if result_count > 0:\n",
    "        feed_url = itunes_result_json[\"results\"][0]['feedUrl']\n",
    "    else:\n",
    "        feed_url = \"None\"\n",
    "    return feed_url\n",
    "\n",
    "def fix_podcast_length(time):\n",
    "    hours = int(time[0:2]) * 60\n",
    "    minutes = int(time[4:5])\n",
    "    length = hours + minutes\n",
    "    return length\n",
    "\n",
    "def df_podcast_episodes(feed_url):\n",
    "    print(feed_url)\n",
    "    if(len(feed_url) > 0):\n",
    "        feed = feedparser.parse(feed_url)\n",
    "        episodes = []\n",
    "        for episode in feed.entries:\n",
    "            info = dict()\n",
    "            info['title'] = episode['title'] if 'title' in episode else ''\n",
    "            info['description']= episode['summary'] if 'summary' in episode else ''\n",
    "            info['length']= fix_podcast_length(episode['itunes_duration']) if 'itunes_duration' in episode else 1\n",
    "            info['date']= pd.to_datetime(episode['published']).date().strftime('%Y-%m-%d') if 'published' in episode else ''\n",
    "            info['medium'] = 'audio'\n",
    "            info['formality'] = 'Intermediate'\n",
    "            info['source'] = feed['feed']['title'] if 'title' in feed['feed'] else ''\n",
    "            info['source_id'] = feed['feed']['title_detail']['base'] if 'title_detail' in feed['feed'] else ''\n",
    "            try:\n",
    "                info['web_url'] = episode['links'][0]['href'] if 'links' in episode else ''\n",
    "            except:\n",
    "                info['web_url'] = feed['feed']['title_detail']['base'] if 'title_detail' in feed['feed'] else ''\n",
    "            info['image_url'] = feed['feed']['image']['href'] if 'image' in feed['feed'] else ''\n",
    "            episodes.append(info)\n",
    "        df = pd.DataFrame(episodes)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No response!\")\n",
    "\n",
    "def add_category_to_audio(df, categories):\n",
    "    all_params = []\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            words = set(re.sub(\"[^\\w]\", \" \",  row.description).split())\n",
    "            intersect = list(words.intersection(categories))\n",
    "            if len(intersect) > 0:\n",
    "                category = intersect[0]\n",
    "            else:\n",
    "                category = 'general'\n",
    "        except:\n",
    "            category = 'general'\n",
    "        all_params.append(category)\n",
    "    df['param'] = all_params\n",
    "    return df\n",
    "\n",
    "def call_podcast_api(categories, podcasts):\n",
    "    empty = pd.DataFrame()\n",
    "    for podcast in podcasts:\n",
    "        if podcast == 'None':\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                url = feed_urls(podcast)\n",
    "                df = df_podcast_episodes(url)\n",
    "                df = add_category_to_audio(df, categories)\n",
    "                empty = empty.append(df, sort=True)\n",
    "                print('SUCCESS!')\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                print('EXCEPTION')\n",
    "                time.sleep(2)\n",
    "    return empty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EBOOKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ebook_date(df):\n",
    "    dates = []\n",
    "    for index, row in df.iterrows():\n",
    "        date = pd.to_datetime(row.date).date().strftime('%Y-%m-%d')\n",
    "        dates.append(date)\n",
    "    df['date'] = dates\n",
    "    return df\n",
    "\n",
    "\n",
    "def ebook_search(search_word, media_value='ebook', entity_value='ebook'):\n",
    "    payload = {'term': search_word, 'media': media_value, 'entity' : entity_value}\n",
    "    itunes_request = requests.get('https://itunes.apple.com/search', params=payload)\n",
    "    itunes_result_json = itunes_request.json()\n",
    "    result_count = itunes_result_json[\"resultCount\"]\n",
    "    if result_count > 0:\n",
    "        df = pd.DataFrame(itunes_result_json['results'])\n",
    "        #NOTE LENGTH IS ACTUALLY THE PRICE BUT USE SAME LABEL FOR CONSISTENCY\n",
    "        df = df.rename(index = str, columns= {'artistName': 'source','trackViewUrl':'web_url',\n",
    "                                        'artworkUrl100':'image_url','price': 'length','releaseDate':'date','trackName':'title'})\n",
    "        df['source_id'] = 'Itunes Ebook'\n",
    "        df['formality'] = 'Formal'\n",
    "        df['medium'] = 'text'\n",
    "        df['param'] = search_word\n",
    "        df = df.fillna(1)\n",
    "        return clean_ebook_date(df)\n",
    "    else:\n",
    "        print('No Results!')\n",
    "        return 'Empty'\n",
    "\n",
    "def call_ebook_api(categories):\n",
    "    empty = pd.DataFrame()\n",
    "    for category in categories:\n",
    "        df = ebook_search(category)\n",
    "        if type(df) != str:\n",
    "            empty = empty.append(df, sort=True)\n",
    "            time.sleep(2)\n",
    "            print('Added '+category)\n",
    "        else:\n",
    "            pass\n",
    "    return empty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movies/Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_search(search_word, media_value='movie', entity_value='movie'):\n",
    "    payload = {'term': search_word, 'media': media_value, 'entity' : entity_value}\n",
    "    itunes_request = requests.get('https://itunes.apple.com/search', params=payload)\n",
    "    itunes_result_json = itunes_request.json()\n",
    "    result_count = itunes_result_json[\"resultCount\"]\n",
    "    return itunes_result_json\n",
    "#     if result_count > 0:\n",
    "#         df = pd.DataFrame(itunes_result_json['results'])\n",
    "#         #NOTE LENGTH IS ACTUALLY THE PRICE BUT USE SAME LABEL FOR CONSISTENCY\n",
    "#         df = df.rename(index = str, columns= {'artistName': 'source','trackViewUrl':'web_url',\n",
    "#                                         'artworkUrl100':'image_url','price': 'length','releaseDate':'date','trackName':'title'})\n",
    "#         df['source_id'] = 'Itunes Ebook'\n",
    "#         df['formality'] = 'Formal'\n",
    "#         df['medium'] = 'text'\n",
    "#         df['param'] = search_word\n",
    "#         df = df.fillna(1)\n",
    "#         return clean_ebook_date(df)\n",
    "#     else:\n",
    "#         print('No Results!')\n",
    "#         return 'Empty'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_search()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
