{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json \n",
    "import requests\n",
    "import datetime\n",
    "from datetime import date\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('max_colwidth', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "newsapi = NewsApiClient(api_key='a539d7df2c7b43e1ac4d12f386d901e8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = newsapi.get_top_headlines(country='us', page_size=50, category = 'health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_api_key = '13bd501bc77542a58e2e6678619b0d60'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = int(str(date.today()).replace('-',''))\n",
    "last_week = int(str(date.today() - datetime.timedelta(days = 14)).replace('-',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYT & NewsAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import date\n",
    "from apikeys import *\n",
    "from info import *\n",
    "\n",
    "#dates to use for API call\n",
    "today = int(str(date.today()).replace('-',''))\n",
    "last_week = int(str(date.today() - datetime.timedelta(days = 14)).replace('-',''))\n",
    "\n",
    "#NEW YORK TIMES\n",
    "#clean the response from NYT API\n",
    "def NYT_title_clean(df):\n",
    "    titles = []\n",
    "    for index, row in df.iterrows():\n",
    "        title = row.headline['main']\n",
    "        titles.append(title)\n",
    "    df['title'] = titles\n",
    "    return df\n",
    "\n",
    "def NYT_dropped_rows(df):\n",
    "    df.pub_date = pd.to_datetime(df.pub_date).dt.date\n",
    "    df.word_count = round(df.word_count / 150)\n",
    "    df.document_type = 'text'\n",
    "    df['formality'] = 'Intermediate'\n",
    "    return df\n",
    "\n",
    "def NYT_dataframe_clean(df):\n",
    "    dataframe = NYT_title_clean(df)\n",
    "    dataframe = NYT_dropped_rows(dataframe)\n",
    "    return dataframe\n",
    "\n",
    "def NYT_api_call_section_based(section, source, page, start, end, key):\n",
    "    url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?fq=section_name:({section_name})&page={page}&source:({source})&begin_date={start}&end_date={end}&api-key={api}'.format(section_name = section, page = page, source = source, start = start, end = end, api = key)\n",
    "    resp = requests.get(url=url)\n",
    "    data = json.loads(resp.text)\n",
    "    df = pd.DataFrame(data['response']['docs'])\n",
    "    df = NYT_dataframe_clean(df)\n",
    "    df['param'] = section\n",
    "    df['image_url'] = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSHEtiVXw8Wi1tp56Nzd5rH_EoOAJA2RInEWvf5h5CQ-6O_YZp7dw'\n",
    "    return df\n",
    "\n",
    "def NYT_api_call_parameter_ALLTIME(param, page, key):\n",
    "    url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q={param}&page={page}&sort=newest&&api-key={api}'.format(param = param, page = page, api = key)\n",
    "    resp = requests.get(url=url)\n",
    "    data = json.loads(resp.text)\n",
    "    df = pd.DataFrame(data['response']['docs'])\n",
    "    df = NYT_dataframe_clean(df)\n",
    "    df['param'] = param\n",
    "    df['image_url'] = 'https://greaterbostonhcs.com/wp-content/uploads/2016/05/Nutrition.jpg'\n",
    "    return df\n",
    "\n",
    "def NYT_pull(categories):\n",
    "    empty = pd.DataFrame()\n",
    "    for word in categories:\n",
    "        try:\n",
    "            df = NYT_api_call_parameter_ALLTIME(word,0,nyt_api_key)\n",
    "            empty = empty.append(df, sort=True)\n",
    "            print('Pulled '+word)\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(word + \" EXCEPTION!!!!\")\n",
    "    empty = empty.drop(['abstract','section_name'],axis = 1)\n",
    "    empty = empty.rename(index=str, columns={\"_id\": \"source_id\", \"document_type\": \"medium\",'pub_date':'date','snippet':'description','word_count':'length'})\n",
    "    return empty\n",
    "\n",
    "#NEWSAPI\n",
    "\n",
    "def rename_columns(df):\n",
    "    df = df.rename(index=str, columns={'publishedAt':'date','url':'web_url','urlToImage':'image_url'})\n",
    "    return df\n",
    "\n",
    "def add_words(df):\n",
    "    lengths = []\n",
    "    for string in df.content:\n",
    "        try:\n",
    "            lengths.append(round(int(string[string.find('+')+1:string.find(' chars')]) / 4 / 250))\n",
    "        except:\n",
    "            lengths.append(4)\n",
    "    return lengths\n",
    "\n",
    "def split_source_info(list_of_dicts):\n",
    "    for item in list_of_dicts:\n",
    "        item['source_id'] = item['source']['id']\n",
    "        item['source'] = item['source']['name']\n",
    "\n",
    "def pull_articles(parameter):\n",
    "    article_results_rel = newsapi.get_everything(q=parameter,sort_by = 'relevancy',language='en', page_size=10, sources=sources_joined)\n",
    "    article_results_rel = article_results_rel['articles']\n",
    "    split_source_info(article_results_rel)\n",
    "    return article_results_rel\n",
    "\n",
    "def clean_articles(list_of_dicts, search_param):\n",
    "    df = pd.DataFrame(list_of_dicts)\n",
    "    try:\n",
    "        df['medium'] = 'text'\n",
    "        df['param'] = search_param\n",
    "        df['publishedAt'] = df['publishedAt'].apply(lambda x: pd.to_datetime(x).date().strftime('%Y-%m-%d'))\n",
    "        df['formality'] = 'Intermediate'\n",
    "        df['length'] = add_words(df)\n",
    "        df = rename_columns(df)\n",
    "        print(search_param)\n",
    "    except:\n",
    "        pass\n",
    "    return df\n",
    "\n",
    "\n",
    "def call_news_api(categories):\n",
    "    empty_df = pd.DataFrame()\n",
    "    for category in categories:\n",
    "        dicts = pull_articles(category)\n",
    "        df = clean_articles(dicts, category)\n",
    "        try:\n",
    "            empty_df = empty_df.append(df, sort=True)\n",
    "        except:\n",
    "            pass\n",
    "        print(len(empty_df.index))\n",
    "    return empty_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from apikeys import *\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import date\n",
    "from info import *\n",
    "\n",
    "auth = tweepy.OAuthHandler(twitter_1, twitter_2)\n",
    "auth.set_access_token(twitter_3, twitter_4)\n",
    "api = tweepy.API(auth)\n",
    "#all twitter handles to scrape\n",
    "# twitter_handles = ['@ATPScience1', '@waitrose', '@MicrobiomeInst', '@veganrecipescom', '@cldiet', '@Onnit', '@vegsoc', '@VeganKosher', '@TheVeganSociety', '@vegan', '@Keto_Recipes_', '@the52diet', '@IFdiet', '@microbiome', '@metagenomics', '@microbiome_news', '@TheGutStuff', '@MyGutHealth', '@PaleoFX',\n",
    "# '@PaleoFoundation', '@ThePaleoDiet', '@PaleoComfort', '@cavemanketo', '@KetoFlu', '@TheKetoKitchen_', '@EatKetoWithMe', '@KetoConnect', '@KetoDietZone', '@Ketogenic', '@USDANutrition', '@FoodRev', '@CSPI', '@simplyrecipes', '@FoodNetwork', '@CookingChannel', '@tasty', '@nytfood', '@finecooking', '@mrcookingpanda'\n",
    "# , '@FODMAPeveryday', '@FODMAPLife', '@FodmappedInfo', '@thefodmapdoctor', '@SimplyGlutenFre', '@gfliving', '@sibotest', '@manjulaskitchen', '@VegTimes', '@CookingLight', '@mealprepwl', '@thehealthygut', '@VitalGutHealth', '@pureguthealth', '@PaleoForBegin', '@PaleoLeap', '@ThePaleoMom', '@paleomagazine', '@PaleoHacks', '@paleogrubs',\n",
    "# '@naturalgourmet', '@Low_Carb_Keto', '@NutritionTwins', '@mckelhill', '@WomensFitnessAu', '@WomensHealthMag', '@MensHealthMag', '@mjfit', '@thugkitchen', '@Leslie_Klenke', '@insidePN', '@ThisMamaCooks', '@EdibleWildFood', '@TheEarthDieter', '@HarvardHealth', '@EverydayHealth', '@DailyHealthTips']\n",
    "\n",
    "#clean response from twitter\n",
    "\n",
    "def clean_tweets(data, categories):\n",
    "    topics = []\n",
    "    for tweet in data:\n",
    "        try:\n",
    "            hashtag = tweet.entities['hashtags'][0]['text']\n",
    "            tags = list(pd.DataFrame(tweet.entities['hashtags']).text)\n",
    "            intersect = list(set(tags).intersection(categories))\n",
    "            if len(intersect) > 0:\n",
    "                hashtag = intersect[0]\n",
    "            else:\n",
    "                hashtag = hashtag\n",
    "        except IndexError:\n",
    "            words = set(re.sub(\"[^\\w]\", \" \",  tweet.text).split())\n",
    "            int2 = list(words.intersection(categories))\n",
    "            if len(int2) > 0:\n",
    "                hashtag = int2[0]\n",
    "            else:\n",
    "                hashtag = 'general'\n",
    "        topics.append(hashtag)\n",
    "    tweets = [{'title':tweet.id, 'date':tweet.created_at.date().strftime('%Y-%m-%d'),\n",
    "       'description': tweet.text, 'source':tweet.user.screen_name,'source_id':tweet.user.id_str,\n",
    "       'formality': 'Informal'\n",
    "       ,'length': 1,'medium':'text'} for tweet in data]\n",
    "    tweets = pd.DataFrame(tweets)\n",
    "    tweets['param'] = topics\n",
    "    return tweets\n",
    "\n",
    "\n",
    "\n",
    "#CALL API\n",
    "def twitter_api_call(list_handles, categories):\n",
    "    empty = pd.DataFrame()\n",
    "    for handle in list_handles:\n",
    "        user_tweets = pd.DataFrame(clean_tweets(api.user_timeline(handle), categories))\n",
    "        empty = empty.append(user_tweets, sort=True)\n",
    "        print(handle)\n",
    "    empty.title = empty.title.astype('str')\n",
    "    empty['web_url'] = 'https://twitter.com/'+empty.source+'/status/'+empty.title\n",
    "    empty['image_url'] = empty['web_url']\n",
    "    return empty\n",
    "import requests\n",
    "\n",
    "class Tweet(object):\n",
    "    def __init__(self, s, embed_str=False):\n",
    "        if not embed_str:\n",
    "            # Use Twitter's oEmbed API\n",
    "            # https://dev.twitter.com/web/embedded-tweets\n",
    "            api = 'https://publish.twitter.com/oembed?url={}'.format(s)\n",
    "            response = requests.get(api)\n",
    "            self.text = response.json()[\"html\"]\n",
    "        else:\n",
    "            self.text = s\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        return self.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apiclient.discovery import build\n",
    "from apiclient.errors import HttpError\n",
    "from oauth2client.tools import argparser\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import pafy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from apikeys import *\n",
    "from info import *\n",
    "\n",
    "\n",
    "DEVELOPER_KEY = youtube_key\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "\n",
    "\n",
    "def clean_youtube_time(string):\n",
    "    if 'H' in string:\n",
    "        minutes = int(string[string.find('H')+1:string.find('M')])\n",
    "        hours = int(string[string.find('T')+1:string.find('H')]) * 60\n",
    "        time = minutes + hours\n",
    "    else:\n",
    "        if 'M' in string:\n",
    "            time = int(string[string.find('T')+1:string.find('M')])\n",
    "        else:\n",
    "            time = 1\n",
    "    return time\n",
    "\n",
    "def youtube_search(q, max_results=10,order=\"date\", token=None, location=None, location_radius=None):\n",
    "\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,developerKey=DEVELOPER_KEY)\n",
    "\n",
    "    search_response = youtube.search().list(\n",
    "    q=q,\n",
    "    type=\"video\",\n",
    "    pageToken=token,\n",
    "    order = order,\n",
    "    part=\"id,snippet\", # Part signifies the different types of data you want\n",
    "    maxResults=max_results,\n",
    "    location=location,\n",
    "    locationRadius=location_radius).execute()\n",
    "\n",
    "    all_dicts = []\n",
    "\n",
    "    for search_result in search_response.get(\"items\", []):\n",
    "        if search_result[\"id\"][\"kind\"] == \"youtube#video\":\n",
    "\n",
    "            title = (search_result['snippet']['title'])\n",
    "\n",
    "            videoId = (search_result['id']['videoId'])\n",
    "\n",
    "            response = youtube.videos().list(\n",
    "            part='statistics, snippet, contentDetails',\n",
    "            id=search_result['id']['videoId']).execute()\n",
    "\n",
    "            channelId = (response['items'][0]['snippet']['channelId'])\n",
    "            channelTitle = (response['items'][0]['snippet']['channelTitle'])\n",
    "            categoryId = (response['items'][0]['snippet']['categoryId'])\n",
    "            favoriteCount = (response['items'][0]['statistics']['favoriteCount'])\n",
    "            viewCount = (response['items'][0]['statistics']['viewCount'])\n",
    "            date = pd.to_datetime((response['items'][0]['snippet']['publishedAt'])).date().strftime('%Y-%m-%d')\n",
    "            description = response['items'][0]['snippet']['localized']['description']\n",
    "            url = 'https://www.youtube.com/watch?v='+videoId\n",
    "            image_url = response['items'][0]['snippet']['thumbnails']['default']['url']\n",
    "            length = clean_youtube_time(response['items'][0]['contentDetails']['duration'])\n",
    "\n",
    "        if 'commentCount' in response['items'][0]['statistics'].keys():\n",
    "            commentCount = (response['items'][0]['statistics']['commentCount'])\n",
    "        else:\n",
    "            commentCount = []\n",
    "\n",
    "        if 'tags' in response['items'][0]['snippet'].keys():\n",
    "            tags = (response['items'][0]['snippet']['tags'])\n",
    "        else:\n",
    "            tags = []\n",
    "\n",
    "        youtube_dict = {'tags':tags,'source_id': channelId,'source': channelTitle,'categoryId':categoryId,'title':title,'videoId':videoId,'viewCount':viewCount,'commentCount':commentCount,'favoriteCount':favoriteCount,\n",
    "                        'formality':'Intermediate', 'medium':'video','date':date, 'description': description, 'web_url':url, 'image_url':image_url, 'length':length}\n",
    "        all_dicts.append(youtube_dict)\n",
    "    return pd.DataFrame(all_dicts)\n",
    "\n",
    "def add_category(df, categories):\n",
    "    # cats = ['keto','ketogenic','paleo','paleolithic','vegan','vegetarian']\n",
    "    all_params = []\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            intersect = list(set(row.tags).intersection(categories))\n",
    "            if len(intersect) > 0:\n",
    "                category = intersect[0]\n",
    "            else:\n",
    "                category = row.tags[0]\n",
    "        except:\n",
    "            category = 'none'\n",
    "        all_params.append(category)\n",
    "    df['param'] = all_params\n",
    "    return df\n",
    "\n",
    "def youtube_api_call(list_accounts, categories):\n",
    "    empty_df = pd.DataFrame()\n",
    "    errors = []\n",
    "    for account in list_accounts:\n",
    "        try:\n",
    "            df = youtube_search(account)\n",
    "            df = add_category(df, categories)\n",
    "            empty_df = empty_df.append(df, sort=True)\n",
    "            print(account)\n",
    "        except:\n",
    "            print(account + \" EXCEPTION!!!!\")\n",
    "    return empty_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FB AND INSTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from instagram.client import InstagramAPI\n",
    "# api = InstagramAPI(client_id=\"EAACl5okwUhQBAD6vhJiELgsatruedytMV67mvDuN2wgXEyRAXG7umE3T0KEweMhlWQWPgku5pF8KBwwOy9JJuFxx2chbYxPTjMdYA2mTd1DL6Jd5t4XwcetZBwhZC1Pyrl5MW2X90w6J9ZCZCWYlODApHzosU7ReUPZBmFITJj0cpsMZCDZATmEKip23ZA1nyA40Dv8IsLccLAZDZD\")\n",
    "# popular_media = api.media_popular(count=20)\n",
    "# for media in popular_media:\n",
    "#     print(media.images['standard_resolution'].url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import facebook\n",
    "\n",
    "graph = facebook.GraphAPI(access_token=\"EAACl5okwUhQBANPTECp6sKqxm3vubZCFmBBEtwdUQgLPAvAFLhpmk6ZAgDTB6Q6g6y6lvz3yjf7iZBW2ELCSrAwDhp28A1xse1QkQrvXowYgivMP0Rz3NRO8cEEy514LC7x3pAmFr1RBZBe7RJVVlsY70OWLHOgceSZCZC0HqzFdw13oEkEZC0Bm4ehPBxTlJFOoyiFE5f8ZCAZDZD\", version=\"2.12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "def feed_urls(search_words, media_value='podcast', entity_value='podcast'):\n",
    "#     Args:\n",
    "#         search_words: The URL-encoded text string to be searched for\n",
    "#         media_value: {movie, podcast, music, musicVideo, audiobook,\n",
    "#                     shortFilm, tvShow, software, ebook, all} optional\n",
    "#                     An optional variable, which indicates the media type to be searched for.\n",
    "#         entity_value: Optional\n",
    "\n",
    "    payload = {'term': search_words, 'media': media_value, 'entity' : entity_value}\n",
    "    itunes_request = requests.get('https://itunes.apple.com/search', params=payload)\n",
    "    itunes_result_json = itunes_request.json()\n",
    "    result_count = itunes_result_json[\"resultCount\"]\n",
    "    if result_count > 0:\n",
    "        feed_url = itunes_result_json[\"results\"][0]['feedUrl']\n",
    "    else:\n",
    "        feed_url = \"None\"\n",
    "    return feed_url\n",
    "\n",
    "def fix_podcast_length(time):\n",
    "    hours = int(time[0:2]) * 60\n",
    "    minutes = int(time[4:5])\n",
    "    length = hours + minutes\n",
    "    return length\n",
    "\n",
    "def df_podcast_episodes(feed_url):\n",
    "    print(feed_url)\n",
    "    if(len(feed_url) > 0):\n",
    "        feed = feedparser.parse(feed_url)\n",
    "        episodes = []\n",
    "        for episode in feed.entries:\n",
    "            info = dict()\n",
    "            info['title'] = episode['title'] if 'title' in episode else ''\n",
    "            info['description']= episode['summary'] if 'summary' in episode else ''\n",
    "            info['length']= fix_podcast_length(episode['itunes_duration']) if 'itunes_duration' in episode else 1\n",
    "            info['date']= pd.to_datetime(episode['published']).date().strftime('%Y-%m-%d') if 'published' in episode else ''\n",
    "            info['medium'] = 'audio'\n",
    "            info['formality'] = 'Intermediate'\n",
    "            info['source'] = feed['feed']['title'] if 'title' in feed['feed'] else ''\n",
    "            info['source_id'] = feed['feed']['title_detail']['base'] if 'title_detail' in feed['feed'] else ''\n",
    "            try:\n",
    "                info['web_url'] = episode['links'][0]['href'] if 'links' in episode else ''\n",
    "            except:\n",
    "                info['web_url'] = feed['feed']['title_detail']['base'] if 'title_detail' in feed['feed'] else ''\n",
    "            info['image_url'] = feed['feed']['image']['href'] if 'image' in feed['feed'] else ''\n",
    "            episodes.append(info)\n",
    "        df = pd.DataFrame(episodes)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No response!\")\n",
    "\n",
    "def add_category_to_audio(df, categories):\n",
    "    all_params = []\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            words = set(re.sub(\"[^\\w]\", \" \",  row.description).split())\n",
    "            intersect = list(words.intersection(categories))\n",
    "            if len(intersect) > 0:\n",
    "                category = intersect[0]\n",
    "            else:\n",
    "                category = 'general'\n",
    "        except:\n",
    "            category = 'general'\n",
    "        all_params.append(category)\n",
    "    df['param'] = all_params\n",
    "    return df\n",
    "\n",
    "def call_podcast_api(categories, podcasts):\n",
    "    empty = pd.DataFrame()\n",
    "    for podcast in podcasts:\n",
    "        if podcast == 'None':\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                url = feed_urls(podcast)\n",
    "                df = df_podcast_episodes(url)\n",
    "                df = add_category_to_audio(df, categories)\n",
    "                empty = empty.append(df, sort=True)\n",
    "                print('SUCCESS!')\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                print('EXCEPTION')\n",
    "                time.sleep(2)\n",
    "    return empty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EBOOKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ebook_date(df):\n",
    "    dates = []\n",
    "    for index, row in df.iterrows():\n",
    "        date = pd.to_datetime(row.date).date().strftime('%Y-%m-%d')\n",
    "        dates.append(date)\n",
    "    df['date'] = dates\n",
    "    return df\n",
    "\n",
    "\n",
    "def ebook_search(search_word, media_value='ebook', entity_value='ebook'):\n",
    "    payload = {'term': search_word, 'media': media_value, 'entity' : entity_value}\n",
    "    itunes_request = requests.get('https://itunes.apple.com/search', params=payload)\n",
    "    itunes_result_json = itunes_request.json()\n",
    "    result_count = itunes_result_json[\"resultCount\"]\n",
    "    if result_count > 0:\n",
    "        df = pd.DataFrame(itunes_result_json['results'])\n",
    "        #NOTE LENGTH IS ACTUALLY THE PRICE BUT USE SAME LABEL FOR CONSISTENCY\n",
    "        df = df.rename(index = str, columns= {'artistName': 'source','trackViewUrl':'web_url',\n",
    "                                        'artworkUrl100':'image_url','price': 'length','releaseDate':'date','trackName':'title'})\n",
    "        df['source_id'] = 'Itunes Ebook'\n",
    "        df['formality'] = 'Formal'\n",
    "        df['medium'] = 'text'\n",
    "        df['param'] = search_word\n",
    "        df = df.fillna(1)\n",
    "        return clean_ebook_date(df)\n",
    "    else:\n",
    "        print('No Results!')\n",
    "        return 'Empty'\n",
    "\n",
    "def call_ebook_api(categories):\n",
    "    empty = pd.DataFrame()\n",
    "    for category in categories:\n",
    "        df = ebook_search(category)\n",
    "        if type(df) != str:\n",
    "            empty = empty.append(df, sort=True)\n",
    "            time.sleep(2)\n",
    "            print('Added '+category)\n",
    "        else:\n",
    "            pass\n",
    "    return empty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movies/Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_category_to_movie(df, categories):\n",
    "    all_params = []\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            words = set(re.sub(\"[^\\w]\", \" \",  row.description).split())\n",
    "            intersect = list(words.intersection(categories))\n",
    "            if len(intersect) > 0:\n",
    "                category = intersect[0]\n",
    "            else:\n",
    "                category = 'general'\n",
    "        except:\n",
    "            category = 'general'\n",
    "        all_params.append(category)\n",
    "    df['param'] = all_params\n",
    "    return df\n",
    "\n",
    "def movie_search(search_word,categories, media_value='movie', entity_value='movie'):\n",
    "    payload = {'term': search_word, 'media': media_value, 'entity' : entity_value}\n",
    "    itunes_request = requests.get('https://itunes.apple.com/search', params=payload)\n",
    "    itunes_result_json = itunes_request.json()\n",
    "    result_count = itunes_result_json[\"resultCount\"]\n",
    "    if result_count > 0:\n",
    "        df = pd.DataFrame(itunes_result_json['results'])\n",
    "        df = df.rename(index = str, columns= {'artistName': 'source','trackViewUrl':'web_url',\n",
    "                                        'artworkUrl100':'image_url','trackTimeMillis': 'length'\n",
    "                                              ,'releaseDate':'date','trackName':'title', 'longDescription':'description'})\n",
    "        df['source_id'] = 'Itunes Movie'\n",
    "        df['formality'] = 'Formal'\n",
    "        df['medium'] = 'video'\n",
    "        df['length'] = round(df['length'] / 60000)\n",
    "        df = df.fillna(1)\n",
    "        df = clean_ebook_date(df)\n",
    "        return add_category_to_movie(df,categories)\n",
    "    else:\n",
    "        print('No Results!')\n",
    "        return 'Empty'\n",
    "\n",
    "def call_movie_api(movies, categories):\n",
    "    empty = pd.DataFrame()\n",
    "    for movie in movies:\n",
    "        try:\n",
    "            df = movie_search(movie, categories)\n",
    "            if type(df) != str:\n",
    "                empty = empty.append(df, sort=True)\n",
    "                time.sleep(5)\n",
    "                print('Added '+movie)\n",
    "            else:\n",
    "                time.sleep(5)\n",
    "        except:\n",
    "            print(movie + ' EXCEPTION')\n",
    "    return empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_list = ['Somebody Feed Phil' , 'Meat Eater' , 'Salt, Fat, Heat, Acid' , 'For Grace' , 'Chef vs Science: The Ultimate Kitchen Challenge' , 'Ugly Delicious' , 'Avec Eric' , 'The Great British Baking Show' , 'Lords and Ladles' , 'Cooking on High' , 'The Mind of a Chef' , 'Anthony Bourdain Parts Unknown' , 'Jeremiah Tower: The Last Magnificent' , 'In Search of Israeli Cuisine' , 'Theater of Life' , 'Sour Grapes' , 'Chefs Table' , 'Noma: My Perfect Storm' , 'The Birth of Saké' , 'Barbecue' , 'A Year in Champagne' , '42 grams' , 'Sugar Coated' , 'Somm' , 'More Than Honey' , 'Jiro Dreams of Sushi' , 'Spinning Plates' , 'The Future of Food' , 'Ingredients' , 'Simply Raw: Reversing Diabetes in 30 Days' , 'Sustainable' , 'A Place at the Table' , 'Farmageddon' , 'Bite Size' , 'Food Chains' , 'Plant Pure Nation' , 'Super Size Me' , 'Food Matters' , 'Food Choices' , 'In Defense of Food' , 'What the Health' , 'GMO OMG' , 'Cowspiracy' , 'Vegucated' , 'Fat, Sick & Nearly Dead' , 'Fed Up' , 'Food Inc.' , 'Hungry for Change' , 'Rotten' , 'Cooked' , 'Forks Over Knives' , 'The Magic Pill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_cats = ['keto',  'mct', 'natural', 'corn-fed', 'cruciferous','paleolithic',\n",
    "'vegetable', 'cooking', 'factory', 'fat', 'anti-biotic', 'grass-fed', 'gluten', 'vegan', 'vegetarian'\n",
    ",'paleo', 'ketogenic', 'leaky gut', 'meal prep', 'micronutrient', 'macronutrient', 'pressure cooked carbs', 'sprouted carbs', 'mct oil', 'coconut oil', 'lean protein', 'fat on nutrion label', 'cholestrol', 'natural sugars', 'diabetes', 'brain fog', 'sourdough', 'bcaas', 'essential amino acids'\n",
    ", 'wild fish', 'farm raised fish', 'corn-fed beef', 'wild game', 'bioavailability', 'amino acids', 'meat', 'grains', 'vitamins', 'supplements', 'sprouts', 'beans', 'legumes', 'lectins', 'night shade', 'auto-immune', 'hormone', 'gluconeogenesis', 'bad fat', 'good fat'\n",
    ", 'root veggies', 'cruciferous vegetables', 'dark leafy greens', 'anti-inflammatory', 'infloamation', 'energy', 'mood', 'gut brain connection', 'wheat belly', 'fermented', 'vegetable oil', 'cooking oils', 'glysophate', 'pesticides', 'factory farming', 'sugar', 'fat loss', 'free range', 'anti-biotic free', 'organic'\n",
    ", 'grass-fed dairy', 'grass-fed protein', 'plant based protein', 'animal protein', 'protein', 'blood glucose', 'fiber', 'starch', 'refined sugar', 'refined carbs', 'simple carbs', 'complex carbs', 'carbs', 'white hdl fat', 'brown ldl fat', 'fats', 'prebiotic', 'enzymes', 'probiotic', 'microbiome'\n",
    ", 'foodmap diet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cats = ['sushi', 'keto']\n",
    "test_movies = ['Jiro Dreams of Sushi', 'The Magic Pill', 'Cooked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Results!\n",
      "No Results!\n",
      "No Results!\n",
      "Added For Grace\n",
      "No Results!\n",
      "No Results!\n",
      "Added Avec Eric\n",
      "No Results!\n",
      "No Results!\n",
      "No Results!\n",
      "No Results!\n",
      "No Results!\n",
      "Added Jeremiah Tower: The Last Magnificent\n",
      "Added In Search of Israeli Cuisine\n",
      "No Results!\n",
      "Added Sour Grapes\n",
      "No Results!\n",
      "Added Noma: My Perfect Storm\n",
      "Added The Birth of Saké\n",
      "Added Barbecue\n",
      "Added A Year in Champagne\n",
      "Added 42 grams\n",
      "No Results!\n",
      "Added Somm\n",
      "Added More Than Honey\n",
      "Added Jiro Dreams of Sushi\n",
      "Added Spinning Plates\n",
      "Added The Future of Food\n",
      "Added Ingredients\n",
      "No Results!\n",
      "Added Sustainable\n",
      "Added A Place at the Table\n",
      "Added Farmageddon\n",
      "Bite Size EXCEPTION\n",
      "Added Food Chains\n",
      "Plant Pure Nation EXCEPTION\n",
      "Added Super Size Me\n",
      "Food Matters EXCEPTION\n",
      "Food Choices EXCEPTION\n",
      "Added In Defense of Food\n",
      "Added What the Health\n",
      "GMO OMG EXCEPTION\n",
      "No Results!\n",
      "Added Vegucated\n",
      "Fat, Sick & Nearly Dead EXCEPTION\n",
      "Fed Up EXCEPTION\n",
      "Added Food Inc.\n",
      "Added Hungry for Change\n",
      "Added Rotten\n",
      "Added Cooked\n",
      "Added Forks Over Knives\n",
      "Added The Magic Pill\n"
     ]
    }
   ],
   "source": [
    "test = call_movie_api(movie_list,nutrition_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cooking',\n",
       " 'diabetes',\n",
       " 'energy',\n",
       " 'fats',\n",
       " 'general',\n",
       " 'meat',\n",
       " 'natural',\n",
       " 'pesticides'}"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(test.param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Michelin', 'JIRO', 'DREAMS', 'located', 'appearances', 'greatest', 'bar', 'world', 'and', 'Jiro', 'rating', 'a', 'kind', 'SUSHI', 'subway', 'story', 'from', '85', 'OF', 'three', 'sushi', 'at', 'out', 'first', 'lovers', 'calling', 'is', 'prestigious', 'Ono', 'seat', 'awarded', 'top', 'coveted', 'Tokyo', 'Sukiyabashi', '10', 'for', 'station', 'repeated', 's', 'by', 'year', 'the', 'proprietor', 'around', 'dollar', 'inauspiciously', 'chef', 'be', 'Despite', 'considered', 'advance', 'to', 'in', 'its', 'pilgrimage', 'months', 'He', 'of', 'humble', 'many', 'star', 'restaurant', 'make', 'shelling', 'only', 'it', 'globe', 'Guide', 'old'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>image_url</th>\n",
       "      <th>artworkUrl30</th>\n",
       "      <th>artworkUrl60</th>\n",
       "      <th>collectionExplicitness</th>\n",
       "      <th>collectionHdPrice</th>\n",
       "      <th>collectionPrice</th>\n",
       "      <th>contentAdvisoryRating</th>\n",
       "      <th>country</th>\n",
       "      <th>currency</th>\n",
       "      <th>kind</th>\n",
       "      <th>description</th>\n",
       "      <th>previewUrl</th>\n",
       "      <th>primaryGenreName</th>\n",
       "      <th>date</th>\n",
       "      <th>trackCensoredName</th>\n",
       "      <th>trackExplicitness</th>\n",
       "      <th>trackHdPrice</th>\n",
       "      <th>trackHdRentalPrice</th>\n",
       "      <th>trackId</th>\n",
       "      <th>title</th>\n",
       "      <th>trackPrice</th>\n",
       "      <th>trackRentalPrice</th>\n",
       "      <th>length</th>\n",
       "      <th>web_url</th>\n",
       "      <th>wrapperType</th>\n",
       "      <th>source_id</th>\n",
       "      <th>formality</th>\n",
       "      <th>medium</th>\n",
       "      <th>param</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>David Gelb</td>\n",
       "      <td>https://is5-ssl.mzstatic.com/image/thumb/Video128/v4/a7/6c/23/a76c2355-b63b-ffeb-1080-5adeabd78fc8/source/100x100bb.jpg</td>\n",
       "      <td>https://is5-ssl.mzstatic.com/image/thumb/Video128/v4/a7/6c/23/a76c2355-b63b-ffeb-1080-5adeabd78fc8/source/30x30bb.jpg</td>\n",
       "      <td>https://is5-ssl.mzstatic.com/image/thumb/Video128/v4/a7/6c/23/a76c2355-b63b-ffeb-1080-5adeabd78fc8/source/60x60bb.jpg</td>\n",
       "      <td>notExplicit</td>\n",
       "      <td>12.99</td>\n",
       "      <td>9.99</td>\n",
       "      <td>PG</td>\n",
       "      <td>USA</td>\n",
       "      <td>USD</td>\n",
       "      <td>feature-movie</td>\n",
       "      <td>JIRO DREAMS OF SUSHI is the story of 85-year-old Jiro Ono, considered by many to be the world’s greatest sushi chef. He is the proprietor of Sukiyabashi Jiro, a 10-seat, sushi-only restaurant inauspiciously located in a Tokyo subway station. Despite its humble appearances, it is the first restaurant of its kind to be awarded a prestigious three-star Michelin Guide rating, and sushi lovers from around the globe make repeated pilgrimage, calling months in advance and shelling out top dollar for a coveted seat at Jiro’s sushi bar.</td>\n",
       "      <td>https://video-ssl.itunes.apple.com/apple-assets-us-std-000001/Video127/v4/19/ed/7a/19ed7a3b-d109-6050-1c74-8a4cb7fc058e/mzvf_7123981860068183185.640x478.h264lc.U.p.m4v</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>2012-03-09</td>\n",
       "      <td>Jiro Dreams of Sushi</td>\n",
       "      <td>notExplicit</td>\n",
       "      <td>12.99</td>\n",
       "      <td>3.99</td>\n",
       "      <td>542088376</td>\n",
       "      <td>Jiro Dreams of Sushi</td>\n",
       "      <td>9.99</td>\n",
       "      <td>3.99</td>\n",
       "      <td>82.0</td>\n",
       "      <td>https://itunes.apple.com/us/movie/jiro-dreams-of-sushi/id542088376?uo=4</td>\n",
       "      <td>track</td>\n",
       "      <td>Itunes Movie</td>\n",
       "      <td>Formal</td>\n",
       "      <td>video</td>\n",
       "      <td>sushi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source  \\\n",
       "0  David Gelb   \n",
       "\n",
       "                                                                                                                 image_url  \\\n",
       "0  https://is5-ssl.mzstatic.com/image/thumb/Video128/v4/a7/6c/23/a76c2355-b63b-ffeb-1080-5adeabd78fc8/source/100x100bb.jpg   \n",
       "\n",
       "                                                                                                            artworkUrl30  \\\n",
       "0  https://is5-ssl.mzstatic.com/image/thumb/Video128/v4/a7/6c/23/a76c2355-b63b-ffeb-1080-5adeabd78fc8/source/30x30bb.jpg   \n",
       "\n",
       "                                                                                                            artworkUrl60  \\\n",
       "0  https://is5-ssl.mzstatic.com/image/thumb/Video128/v4/a7/6c/23/a76c2355-b63b-ffeb-1080-5adeabd78fc8/source/60x60bb.jpg   \n",
       "\n",
       "  collectionExplicitness  collectionHdPrice  collectionPrice  \\\n",
       "0            notExplicit              12.99             9.99   \n",
       "\n",
       "  contentAdvisoryRating country currency           kind  \\\n",
       "0                    PG     USA      USD  feature-movie   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             description  \\\n",
       "0  JIRO DREAMS OF SUSHI is the story of 85-year-old Jiro Ono, considered by many to be the world’s greatest sushi chef. He is the proprietor of Sukiyabashi Jiro, a 10-seat, sushi-only restaurant inauspiciously located in a Tokyo subway station. Despite its humble appearances, it is the first restaurant of its kind to be awarded a prestigious three-star Michelin Guide rating, and sushi lovers from around the globe make repeated pilgrimage, calling months in advance and shelling out top dollar for a coveted seat at Jiro’s sushi bar.   \n",
       "\n",
       "                                                                                                                                                                previewUrl  \\\n",
       "0  https://video-ssl.itunes.apple.com/apple-assets-us-std-000001/Video127/v4/19/ed/7a/19ed7a3b-d109-6050-1c74-8a4cb7fc058e/mzvf_7123981860068183185.640x478.h264lc.U.p.m4v   \n",
       "\n",
       "  primaryGenreName        date     trackCensoredName trackExplicitness  \\\n",
       "0      Documentary  2012-03-09  Jiro Dreams of Sushi       notExplicit   \n",
       "\n",
       "   trackHdPrice  trackHdRentalPrice    trackId                 title  \\\n",
       "0         12.99                3.99  542088376  Jiro Dreams of Sushi   \n",
       "\n",
       "   trackPrice  trackRentalPrice  length  \\\n",
       "0        9.99              3.99    82.0   \n",
       "\n",
       "                                                                   web_url  \\\n",
       "0  https://itunes.apple.com/us/movie/jiro-dreams-of-sushi/id542088376?uo=4   \n",
       "\n",
       "  wrapperType     source_id formality medium  param  \n",
       "0       track  Itunes Movie    Formal  video  sushi  "
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_search('Jiro Dreams of Sushi', test_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
