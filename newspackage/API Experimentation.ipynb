{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json \n",
    "import requests\n",
    "import datetime\n",
    "from datetime import date\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('max_colwidth', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "newsapi = NewsApiClient(api_key='a539d7df2c7b43e1ac4d12f386d901e8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = newsapi.get_top_headlines(country='us', page_size=50, category = 'health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_api_key = '13bd501bc77542a58e2e6678619b0d60'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = int(str(date.today()).replace('-',''))\n",
    "last_week = int(str(date.today() - datetime.timedelta(days = 14)).replace('-',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYT & NewsAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import date\n",
    "from apikeys import *\n",
    "from info import *\n",
    "\n",
    "#dates to use for API call\n",
    "today = int(str(date.today()).replace('-',''))\n",
    "last_week = int(str(date.today() - datetime.timedelta(days = 14)).replace('-',''))\n",
    "\n",
    "#NEW YORK TIMES\n",
    "#clean the response from NYT API\n",
    "def NYT_title_clean(df):\n",
    "    titles = []\n",
    "    for index, row in df.iterrows():\n",
    "        title = row.headline['main']\n",
    "        titles.append(title)\n",
    "    df['title'] = titles\n",
    "    return df\n",
    "\n",
    "def NYT_dropped_rows(df):\n",
    "    df.pub_date = pd.to_datetime(df.pub_date).dt.date\n",
    "    df.word_count = round(df.word_count / 150)\n",
    "    df.document_type = 'text'\n",
    "    df['formality'] = 'Intermediate'\n",
    "    return df\n",
    "\n",
    "def NYT_dataframe_clean(df):\n",
    "    dataframe = NYT_title_clean(df)\n",
    "    dataframe = NYT_dropped_rows(dataframe)\n",
    "    return dataframe\n",
    "\n",
    "def NYT_api_call_section_based(section, source, page, start, end, key):\n",
    "    url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?fq=section_name:({section_name})&page={page}&source:({source})&begin_date={start}&end_date={end}&api-key={api}'.format(section_name = section, page = page, source = source, start = start, end = end, api = key)\n",
    "    resp = requests.get(url=url)\n",
    "    data = json.loads(resp.text)\n",
    "    df = pd.DataFrame(data['response']['docs'])\n",
    "    df = NYT_dataframe_clean(df)\n",
    "    df['param'] = section\n",
    "    df['image_url'] = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSHEtiVXw8Wi1tp56Nzd5rH_EoOAJA2RInEWvf5h5CQ-6O_YZp7dw'\n",
    "    return df\n",
    "\n",
    "def NYT_api_call_parameter_ALLTIME(param, page, key):\n",
    "    url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q={param}&page={page}&sort=newest&&api-key={api}'.format(param = param, page = page, api = key)\n",
    "    resp = requests.get(url=url)\n",
    "    data = json.loads(resp.text)\n",
    "    df = pd.DataFrame(data['response']['docs'])\n",
    "    df = NYT_dataframe_clean(df)\n",
    "    df['param'] = param\n",
    "    df['image_url'] = 'https://greaterbostonhcs.com/wp-content/uploads/2016/05/Nutrition.jpg'\n",
    "    return df\n",
    "\n",
    "def NYT_pull(categories):\n",
    "    empty = pd.DataFrame()\n",
    "    for word in categories:\n",
    "        try:\n",
    "            df = NYT_api_call_parameter_ALLTIME(word,0,nyt_api_key)\n",
    "            empty = empty.append(df, sort=True)\n",
    "            print('Pulled '+word)\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(word + \" EXCEPTION!!!!\")\n",
    "    empty = empty.drop(['abstract','section_name'],axis = 1)\n",
    "    empty = empty.rename(index=str, columns={\"_id\": \"source_id\", \"document_type\": \"medium\",'pub_date':'date','snippet':'description','word_count':'length'})\n",
    "    return empty\n",
    "\n",
    "#NEWSAPI\n",
    "\n",
    "def rename_columns(df):\n",
    "    df = df.rename(index=str, columns={'publishedAt':'date','url':'web_url','urlToImage':'image_url'})\n",
    "    return df\n",
    "\n",
    "def add_words(df):\n",
    "    lengths = []\n",
    "    for string in df.content:\n",
    "        try:\n",
    "            lengths.append(round(int(string[string.find('+')+1:string.find(' chars')]) / 4 / 250))\n",
    "        except:\n",
    "            lengths.append(4)\n",
    "    return lengths\n",
    "\n",
    "def split_source_info(list_of_dicts):\n",
    "    for item in list_of_dicts:\n",
    "        item['source_id'] = item['source']['id']\n",
    "        item['source'] = item['source']['name']\n",
    "\n",
    "def pull_articles(parameter):\n",
    "    article_results_rel = newsapi.get_everything(q=parameter,sort_by = 'relevancy',language='en', page_size=10, sources=sources_joined)\n",
    "    article_results_rel = article_results_rel['articles']\n",
    "    split_source_info(article_results_rel)\n",
    "    return article_results_rel\n",
    "\n",
    "def clean_articles(list_of_dicts, search_param):\n",
    "    df = pd.DataFrame(list_of_dicts)\n",
    "    try:\n",
    "        df['medium'] = 'text'\n",
    "        df['param'] = search_param\n",
    "        df['publishedAt'] = df['publishedAt'].apply(lambda x: pd.to_datetime(x).date().strftime('%Y-%m-%d'))\n",
    "        df['formality'] = 'Intermediate'\n",
    "        df['length'] = add_words(df)\n",
    "        df = rename_columns(df)\n",
    "        print(search_param)\n",
    "    except:\n",
    "        pass\n",
    "    return df\n",
    "\n",
    "\n",
    "def call_news_api(categories):\n",
    "    empty_df = pd.DataFrame()\n",
    "    for category in categories:\n",
    "        dicts = pull_articles(category)\n",
    "        df = clean_articles(dicts, category)\n",
    "        try:\n",
    "            empty_df = empty_df.append(df, sort=True)\n",
    "        except:\n",
    "            pass\n",
    "        print(len(empty_df.index))\n",
    "    return empty_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from apikeys import *\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import date\n",
    "from info import *\n",
    "\n",
    "auth = tweepy.OAuthHandler(twitter_1, twitter_2)\n",
    "auth.set_access_token(twitter_3, twitter_4)\n",
    "api = tweepy.API(auth)\n",
    "#all twitter handles to scrape\n",
    "# twitter_handles = ['@ATPScience1', '@waitrose', '@MicrobiomeInst', '@veganrecipescom', '@cldiet', '@Onnit', '@vegsoc', '@VeganKosher', '@TheVeganSociety', '@vegan', '@Keto_Recipes_', '@the52diet', '@IFdiet', '@microbiome', '@metagenomics', '@microbiome_news', '@TheGutStuff', '@MyGutHealth', '@PaleoFX',\n",
    "# '@PaleoFoundation', '@ThePaleoDiet', '@PaleoComfort', '@cavemanketo', '@KetoFlu', '@TheKetoKitchen_', '@EatKetoWithMe', '@KetoConnect', '@KetoDietZone', '@Ketogenic', '@USDANutrition', '@FoodRev', '@CSPI', '@simplyrecipes', '@FoodNetwork', '@CookingChannel', '@tasty', '@nytfood', '@finecooking', '@mrcookingpanda'\n",
    "# , '@FODMAPeveryday', '@FODMAPLife', '@FodmappedInfo', '@thefodmapdoctor', '@SimplyGlutenFre', '@gfliving', '@sibotest', '@manjulaskitchen', '@VegTimes', '@CookingLight', '@mealprepwl', '@thehealthygut', '@VitalGutHealth', '@pureguthealth', '@PaleoForBegin', '@PaleoLeap', '@ThePaleoMom', '@paleomagazine', '@PaleoHacks', '@paleogrubs',\n",
    "# '@naturalgourmet', '@Low_Carb_Keto', '@NutritionTwins', '@mckelhill', '@WomensFitnessAu', '@WomensHealthMag', '@MensHealthMag', '@mjfit', '@thugkitchen', '@Leslie_Klenke', '@insidePN', '@ThisMamaCooks', '@EdibleWildFood', '@TheEarthDieter', '@HarvardHealth', '@EverydayHealth', '@DailyHealthTips']\n",
    "\n",
    "#clean response from twitter\n",
    "\n",
    "def clean_tweets(data, categories):\n",
    "    tweets = []\n",
    "    for tweet in data:\n",
    "        try:\n",
    "            hashtag = tweet.entities['hashtags'][0]['text']\n",
    "            tags = list(pd.DataFrame(tweet.entities['hashtags']).text)\n",
    "            intersect = list(set(tags).intersection(categories))\n",
    "            if len(intersect) > 0:\n",
    "                hashtag = intersect[0]\n",
    "            else:\n",
    "                hashtag = hashtag\n",
    "        except IndexError:\n",
    "            hashtag = 'general'\n",
    "        tweets = [{'title':tweet.id, 'date':tweet.created_at.date().strftime('%Y-%m-%d'),\n",
    "           'description': tweet.text, 'source':tweet.user.screen_name,'source_id':tweet.user.id_str,\n",
    "           'formality': 'Informal'\n",
    "           ,'length': 1,'medium':'text', 'param':hashtag} for tweet in data]\n",
    "    tweets = pd.DataFrame(tweets)\n",
    "    return tweets\n",
    "\n",
    "#CALL API\n",
    "def twitter_api_call(list_handles, categories):\n",
    "    empty = pd.DataFrame()\n",
    "    for handle in list_handles:\n",
    "        user_tweets = pd.DataFrame(clean_tweets(api.user_timeline(handle), categories))\n",
    "        empty = empty.append(user_tweets, sort=True)\n",
    "        print(handle)\n",
    "    empty.title = empty.title.astype('str')\n",
    "    empty['web_url'] = 'https://twitter.com/'+empty.source+'/status/'+empty.title\n",
    "    empty['image_url'] = empty['web_url']\n",
    "    return empty\n",
    "import requests\n",
    "\n",
    "class Tweet(object):\n",
    "    def __init__(self, s, embed_str=False):\n",
    "        if not embed_str:\n",
    "            # Use Twitter's oEmbed API\n",
    "            # https://dev.twitter.com/web/embedded-tweets\n",
    "            api = 'https://publish.twitter.com/oembed?url={}'.format(s)\n",
    "            response = requests.get(api)\n",
    "            self.text = response.json()[\"html\"]\n",
    "        else:\n",
    "            self.text = s\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        return self.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apiclient.discovery import build\n",
    "from apiclient.errors import HttpError\n",
    "from oauth2client.tools import argparser\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import pafy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from apikeys import *\n",
    "from info import *\n",
    "\n",
    "\n",
    "DEVELOPER_KEY = youtube_key\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "\n",
    "\n",
    "def clean_youtube_time(string):\n",
    "    if 'H' in string:\n",
    "        minutes = int(string[string.find('H')+1:string.find('M')])\n",
    "        hours = int(string[string.find('T')+1:string.find('H')]) * 60\n",
    "        time = minutes + hours\n",
    "    else:\n",
    "        if 'M' in string:\n",
    "            time = int(string[string.find('T')+1:string.find('M')])\n",
    "        else:\n",
    "            time = 1\n",
    "    return time\n",
    "\n",
    "def youtube_search(q, max_results=10,order=\"date\", token=None, location=None, location_radius=None):\n",
    "\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,developerKey=DEVELOPER_KEY)\n",
    "\n",
    "    search_response = youtube.search().list(\n",
    "    q=q,\n",
    "    type=\"video\",\n",
    "    pageToken=token,\n",
    "    order = order,\n",
    "    part=\"id,snippet\", # Part signifies the different types of data you want\n",
    "    maxResults=max_results,\n",
    "    location=location,\n",
    "    locationRadius=location_radius).execute()\n",
    "\n",
    "    all_dicts = []\n",
    "\n",
    "    for search_result in search_response.get(\"items\", []):\n",
    "        if search_result[\"id\"][\"kind\"] == \"youtube#video\":\n",
    "\n",
    "            title = (search_result['snippet']['title'])\n",
    "\n",
    "            videoId = (search_result['id']['videoId'])\n",
    "\n",
    "            response = youtube.videos().list(\n",
    "            part='statistics, snippet, contentDetails',\n",
    "            id=search_result['id']['videoId']).execute()\n",
    "\n",
    "            channelId = (response['items'][0]['snippet']['channelId'])\n",
    "            channelTitle = (response['items'][0]['snippet']['channelTitle'])\n",
    "            categoryId = (response['items'][0]['snippet']['categoryId'])\n",
    "            favoriteCount = (response['items'][0]['statistics']['favoriteCount'])\n",
    "            viewCount = (response['items'][0]['statistics']['viewCount'])\n",
    "            date = pd.to_datetime((response['items'][0]['snippet']['publishedAt'])).date().strftime('%Y-%m-%d')\n",
    "            description = response['items'][0]['snippet']['localized']['description']\n",
    "            url = 'https://www.youtube.com/watch?v='+videoId\n",
    "            image_url = response['items'][0]['snippet']['thumbnails']['default']['url']\n",
    "            length = clean_youtube_time(response['items'][0]['contentDetails']['duration'])\n",
    "\n",
    "        if 'commentCount' in response['items'][0]['statistics'].keys():\n",
    "            commentCount = (response['items'][0]['statistics']['commentCount'])\n",
    "        else:\n",
    "            commentCount = []\n",
    "\n",
    "        if 'tags' in response['items'][0]['snippet'].keys():\n",
    "            tags = (response['items'][0]['snippet']['tags'])\n",
    "        else:\n",
    "            tags = []\n",
    "\n",
    "        youtube_dict = {'tags':tags,'source_id': channelId,'source': channelTitle,'categoryId':categoryId,'title':title,'videoId':videoId,'viewCount':viewCount,'commentCount':commentCount,'favoriteCount':favoriteCount,\n",
    "                        'formality':'Intermediate', 'medium':'video','date':date, 'description': description, 'web_url':url, 'image_url':image_url, 'length':length}\n",
    "        all_dicts.append(youtube_dict)\n",
    "    return pd.DataFrame(all_dicts)\n",
    "\n",
    "def add_category(df, categories):\n",
    "    # cats = ['keto','ketogenic','paleo','paleolithic','vegan','vegetarian']\n",
    "    all_params = []\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            intersect = list(set(row.tags).intersection(categories))\n",
    "            if len(intersect) > 0:\n",
    "                category = intersect[0]\n",
    "            else:\n",
    "                category = row.tags[0]\n",
    "        except:\n",
    "            category = 'none'\n",
    "        all_params.append(category)\n",
    "    df['param'] = all_params\n",
    "    return df\n",
    "\n",
    "def youtube_api_call(list_accounts, categories):\n",
    "    empty_df = pd.DataFrame()\n",
    "    errors = []\n",
    "    for account in list_accounts:\n",
    "        try:\n",
    "            df = youtube_search(account)\n",
    "            df = add_category(df, categories)\n",
    "            empty_df = empty_df.append(df, sort=True)\n",
    "            print(account)\n",
    "        except:\n",
    "            print(account + \" EXCEPTION!!!!\")\n",
    "    return empty_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FB AND INSTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from instagram.client import InstagramAPI\n",
    "# api = InstagramAPI(client_id=\"EAACl5okwUhQBAD6vhJiELgsatruedytMV67mvDuN2wgXEyRAXG7umE3T0KEweMhlWQWPgku5pF8KBwwOy9JJuFxx2chbYxPTjMdYA2mTd1DL6Jd5t4XwcetZBwhZC1Pyrl5MW2X90w6J9ZCZCWYlODApHzosU7ReUPZBmFITJj0cpsMZCDZATmEKip23ZA1nyA40Dv8IsLccLAZDZD\")\n",
    "# popular_media = api.media_popular(count=20)\n",
    "# for media in popular_media:\n",
    "#     print(media.images['standard_resolution'].url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import facebook\n",
    "\n",
    "graph = facebook.GraphAPI(access_token=\"EAACl5okwUhQBANPTECp6sKqxm3vubZCFmBBEtwdUQgLPAvAFLhpmk6ZAgDTB6Q6g6y6lvz3yjf7iZBW2ELCSrAwDhp28A1xse1QkQrvXowYgivMP0Rz3NRO8cEEy514LC7x3pAmFr1RBZBe7RJVVlsY70OWLHOgceSZCZC0HqzFdw13oEkEZC0Bm4ehPBxTlJFOoyiFE5f8ZCAZDZD\", version=\"2.12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedUrlLinks(search_words, media_value='podcast', entity_value='podcast'):\n",
    "    \"\"\"\n",
    "    Fetches FeedUrl(s) of a search request made using the Itunes Api\n",
    "    \n",
    "    Args:\n",
    "        search_words: The URL-encoded text string to be searched for\n",
    "        media_value: {movie, podcast, music, musicVideo, audiobook,\n",
    "                    shortFilm, tvShow, software, ebook, all} optional\n",
    "                    An optional variable, which indicates the media type to be searched for.\n",
    "        entity_value: Optional\n",
    "        \n",
    "    Returns:\n",
    "        A list of feed urls\n",
    "        \n",
    "        example:\n",
    "        \n",
    "        ['https://www.npr.org/rss/podcast.php?id=381444908', \n",
    "        'http://www.spacemusic.nl/podcast/freshairlounge/freshairlounge.xml', \n",
    "        'http://feeds.soundcloud.com/users/soundcloud:users:192634952/sounds.rss', \n",
    "        'http://feeds.feedburner.com/FAIOR_vid', \n",
    "        'http://feeds.soundcloud.com/users/soundcloud:users:280924508/sounds.rss',\n",
    "        'http://feeds2.feedburner.com/faior', 'http://rss.acast.com/takingthepulse']\n",
    "\n",
    "    \"\"\"\n",
    "    payload = {'term': search_words, 'media': media_value, 'entity' : entity_value}\n",
    "    # make a http request with the payload as query parameters\n",
    "    itunes_request = requests.get('https://itunes.apple.com/search', params=payload)\n",
    "    # print Full request query\n",
    "    print(\"\\nYour Passed in query is ->\", itunes_request.url)\n",
    "    \n",
    "    # Store the json result of the Query\n",
    "    itunes_result_json = itunes_request.json()\n",
    "#     return itunes_result_json\n",
    "    #print(\"\\n\",itunes_result_json)\n",
    "    # Get the number of results count so we know what to loop through\n",
    "    result_count = itunes_result_json[\"resultCount\"]\n",
    "    feed_url_list = []\n",
    "    i = 0\n",
    "    while(i < result_count):\n",
    "        # get the Feed Url for each result\n",
    "        feed_url = itunes_result_json[\"results\"][i]['feedUrl']\n",
    "        feed_url_list.append(feed_url)\n",
    "        i = i + 1\n",
    "    return feed_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your Passed in query is -> https://itunes.apple.com/search?term=wellness+mama&media=podcast&entity=podcast\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://wellnessmama.com/feed/podcast',\n",
       " 'http://theenginemom.libsyn.com/rss',\n",
       " 'http://mamainthemaking.libsyn.com/rss',\n",
       " 'https://unconventionalwellnessradio.castos.com/feed']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedUrlLinks('wellness mama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your Passed in query is -> https://itunes.apple.com/search?term=wellness+mama&media=podcast&entity=podcast\n",
      "\n",
      " ['https://wellnessmama.com/feed/podcast', 'http://theenginemom.libsyn.com/rss', 'http://mamainthemaking.libsyn.com/rss', 'https://unconventionalwellnessradio.castos.com/feed']\n",
      "url name https://wellnessmama.com/feed/podcast\n",
      "url name http://theenginemom.libsyn.com/rss\n",
      "url name http://mamainthemaking.libsyn.com/rss\n",
      "url name https://unconventionalwellnessradio.castos.com/feed\n"
     ]
    }
   ],
   "source": [
    "# In the future ask the User for the name of the Podcast to be searched for\n",
    "# construct the Itunes search query to search for the Podcast\n",
    "\n",
    "\n",
    "feed_url_link_list = feedUrlLinks(\"wellness mama\")\n",
    "\n",
    "if(len(feed_url_link_list) > 0):\n",
    "    print(\"\\n\", feed_url_link_list)\n",
    "    i = 1\n",
    "    for url in feed_url_link_list:\n",
    "        feed = feedparser.parse(url)\n",
    "        # (feed.entries) is still the same as feed['entries']\n",
    "        print('url name', url)\n",
    "        sheet_list = []\n",
    "        for item in feed.entries:\n",
    "            info = dict()\n",
    "            info['title'] = item['title'] if 'title' in item else ''\n",
    "            info['Episode description']= item['summary'] if 'summary' in item else ''\n",
    "            info['Episode length']= item['itunes_duration'] if 'itunes_duration' in item else ''\n",
    "            info['Release date']= item['published'] if 'published' in item else ''\n",
    "            sheet_list.append(info)\n",
    "        df = pd.DataFrame(sheet_list)\n",
    "        i = i + 1\n",
    "else:\n",
    "    print(\"Search came back as empty, can try again with a different keyword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['itunes_title', 'title', 'title_detail', 'authors', 'author', 'author_detail', 'subtitle', 'subtitle_detail', 'summary', 'summary_detail', 'content', 'googleplay_description', 'links', 'id', 'guidislink', 'link', 'published', 'published_parsed'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed.entries[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time.struct_time(tm_year=2018, tm_mon=11, tm_mday=14, tm_hour=20, tm_min=53, tm_sec=0, tm_wday=2, tm_yday=318, tm_isdst=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed.entries[0]['published_parsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
